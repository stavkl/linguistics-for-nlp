{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "morphbert",
      "language": "python",
      "name": "morphbert"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "sigmorphon-walkthrough.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rd_hz3ZpQvdo"
      },
      "source": [
        "# \"Getting the life out of Living\"\n",
        "> \"A walkthrough creating, fine-tuning and evaluating the data for the SIGMORPHON 2020 paper\"\n",
        "\n",
        "- toc:false\n",
        "- branch: master\n",
        "- badges: true\n",
        "- comments: true\n",
        "- author: Stav Klein\n",
        "- categories: [fastpages, jupyter]\n",
        "- image: images/draw-bert.png"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YbENcnstCJ7n"
      },
      "source": [
        "### Importing the libraries\n",
        "Note that the 'bclm' library is an internal library developed in the ONLP lab, and is currently unavailable for public use. All the results are obtailable and reproducible with the standard parsing of CONLL formatted files (all available in the ONLP Github page)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CsEIErVHCJ7o",
        "outputId": "555910f3-e912-479c-dc58-8f4bf83b62d4"
      },
      "source": [
        "import os\n",
        "import csv\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm, trange\n",
        "import bclm\n",
        "\n",
        "import torch\n",
        "from torch.optim import Adam\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from transformers import BertTokenizer, BertConfig\n",
        "from transformers import BertForTokenClassification, AdamW"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JyVemxq7CJ7t"
      },
      "source": [
        "### Manually setting seeds\n",
        "Results are calculated based on an average of 5 independent runs of this code. The seeds are set to minimize the variation between runs. Due to internal randomization in pytorch results can't be identical from run to run even when using the same seeds (see [pytorch documentation](https://pytorch.org/docs/stable/notes/randomness.html))"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CxsmOuLlCJ7t"
      },
      "source": [
        "torch.manual_seed(3)\n",
        "np.random.seed(3)\n",
        "torch.cuda.manual_seed_all(3)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1uO7mV-FCJ7w"
      },
      "source": [
        "### Taking a first and important look at the data\n",
        "Data is split to train, dev and test sets. We fine-tune the model on the train set and evaluate it on the dev set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E6SHIDLsCJ7w"
      },
      "source": [
        "train = bclm.read_dataframe('spmrl', subset='train')\n",
        "train_df = bclm.get_token_df(train, ['upostag'])\n",
        "train_df['token_str'] = train_df['token_str'].str.replace('”','\"')\n",
        "\n",
        "dev = bclm.read_dataframe('spmrl', subset='dev')\n",
        "dev_df = bclm.get_token_df(dev, ['upostag'])\n",
        "dev_df['token_str'] = dev_df['token_str'].str.replace('”','\"')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_0IOgMJfEUOq"
      },
      "source": [
        "### Here are the first lines of the dev set\n",
        "Notice that some words have a multi-tag while other words carry a simple tag - a challenge for Hebrew POS-tagging."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "sDthGvqnCJ7z",
        "outputId": "27301dad-13fa-4c86-b6d5-8865375fe2fc"
      },
      "source": [
        "dev_df.head(20)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sent_id</th>\n",
              "      <th>token_id</th>\n",
              "      <th>token_str</th>\n",
              "      <th>upostag</th>\n",
              "      <th>set</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>עשרות</td>\n",
              "      <td>CDT</td>\n",
              "      <td>dev</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>אנשים</td>\n",
              "      <td>NN</td>\n",
              "      <td>dev</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>מגיעים</td>\n",
              "      <td>BN</td>\n",
              "      <td>dev</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>מתאילנד</td>\n",
              "      <td>PREPOSITION^NNP</td>\n",
              "      <td>dev</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>לישראל</td>\n",
              "      <td>PREPOSITION^NNP</td>\n",
              "      <td>dev</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>כשהם</td>\n",
              "      <td>TEMP^PRP</td>\n",
              "      <td>dev</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>נרשמים</td>\n",
              "      <td>BN</td>\n",
              "      <td>dev</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>כמתנדבים</td>\n",
              "      <td>PREPOSITION^NN</td>\n",
              "      <td>dev</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>1</td>\n",
              "      <td>9</td>\n",
              "      <td>,</td>\n",
              "      <td>yyCM</td>\n",
              "      <td>dev</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>1</td>\n",
              "      <td>10</td>\n",
              "      <td>אך</td>\n",
              "      <td>CC</td>\n",
              "      <td>dev</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>1</td>\n",
              "      <td>11</td>\n",
              "      <td>למעשה</td>\n",
              "      <td>RB</td>\n",
              "      <td>dev</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>1</td>\n",
              "      <td>12</td>\n",
              "      <td>משמשים</td>\n",
              "      <td>BN</td>\n",
              "      <td>dev</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>1</td>\n",
              "      <td>13</td>\n",
              "      <td>עובדים</td>\n",
              "      <td>NN</td>\n",
              "      <td>dev</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>1</td>\n",
              "      <td>14</td>\n",
              "      <td>שכירים</td>\n",
              "      <td>JJ</td>\n",
              "      <td>dev</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>1</td>\n",
              "      <td>15</td>\n",
              "      <td>זולים</td>\n",
              "      <td>JJ</td>\n",
              "      <td>dev</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>1</td>\n",
              "      <td>16</td>\n",
              "      <td>.</td>\n",
              "      <td>yyDOT</td>\n",
              "      <td>dev</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>תופעה</td>\n",
              "      <td>NN</td>\n",
              "      <td>dev</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>זו</td>\n",
              "      <td>PRP</td>\n",
              "      <td>dev</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>התבררה</td>\n",
              "      <td>VB</td>\n",
              "      <td>dev</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>אתמול</td>\n",
              "      <td>RB</td>\n",
              "      <td>dev</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    sent_id  token_id token_str          upostag  set\n",
              "0         1         1     עשרות              CDT  dev\n",
              "1         1         2     אנשים               NN  dev\n",
              "2         1         3    מגיעים               BN  dev\n",
              "3         1         4   מתאילנד  PREPOSITION^NNP  dev\n",
              "4         1         5    לישראל  PREPOSITION^NNP  dev\n",
              "5         1         6      כשהם         TEMP^PRP  dev\n",
              "6         1         7    נרשמים               BN  dev\n",
              "7         1         8  כמתנדבים   PREPOSITION^NN  dev\n",
              "8         1         9         ,             yyCM  dev\n",
              "9         1        10        אך               CC  dev\n",
              "10        1        11     למעשה               RB  dev\n",
              "11        1        12    משמשים               BN  dev\n",
              "12        1        13    עובדים               NN  dev\n",
              "13        1        14    שכירים               JJ  dev\n",
              "14        1        15     זולים               JJ  dev\n",
              "15        1        16         .            yyDOT  dev\n",
              "16        2         1     תופעה               NN  dev\n",
              "17        2         2        זו              PRP  dev\n",
              "18        2         3    התבררה               VB  dev\n",
              "19        2         4     אתמול               RB  dev"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2QT4gbECCJ71"
      },
      "source": [
        "### Uniform column names\n",
        "We evaluated on different datasets so a uniform column name was needed"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iVS5anvrCJ72"
      },
      "source": [
        "train_df.rename(columns = {\"token_str\": \"form\"}, inplace = True)\n",
        "dev_df.rename(columns = {\"token_str\": \"form\"}, inplace = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-ZpljCECJ8C"
      },
      "source": [
        "### Get lists of sentences and their corresponding labels\n",
        "Includes an example of a sentence from the train set at the end. Also, not shown here, we removed the four longest sentences from the dev set, as they caused systematic issued later on in the code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lRkGMXYrCJ8D",
        "outputId": "da09bbcc-0b4d-49ba-db3e-bf0d2c6b202a"
      },
      "source": [
        "class sentenceGetter(object):\n",
        "    def __init__(self, data, max_sent=None):\n",
        "        self.index = 0\n",
        "        self.max_sent = max_sent\n",
        "        self.tokens = data['form']\n",
        "        self.labels = data['upostag']\n",
        "        #for evaluating by word-accuracy\n",
        "        self.correspondingToken = data['token_id']\n",
        "        self.orig_sent_id = data['sent_id']\n",
        "    \n",
        "    def sentences(self):\n",
        "        sent = []\n",
        "        counter = 0\n",
        "        \n",
        "        for token,label, corres_tok, sent_id in zip(self.tokens, self.labels, self.correspondingToken, self.orig_sent_id):\n",
        "            sent.append((token, label, corres_tok, sent_id))\n",
        "            if token.strip() == \".\":\n",
        "                yield sent\n",
        "                sent = []\n",
        "                counter += 1\n",
        "            if self.max_sent is not None and counter >= self.max_sent:\n",
        "                return\n",
        "\n",
        "train_getter = sentenceGetter(train_df)\n",
        "dev_getter = sentenceGetter(dev_df)\n",
        "test_getter = sentenceGetter(test_df)\n",
        "\n",
        "train_sentences = [[token for token, label, corres_tok, sent_id in sent] for sent in train_getter.sentences()]\n",
        "train_labels = [[label for token, label, corres_tok, sent_id in sent] for sent in train_getter.sentences()]\n",
        "\n",
        "dev_sentences = [[token for token, label, corres_tok, sent_id in sent] for sent in dev_getter.sentences()]\n",
        "dev_labels = [[label for token, label, corres_tok, sent_id in sent] for sent in dev_getter.sentences()]\n",
        "dev_corresTokens = [[corres_tok for token, label, corres_tok, sent_id in sent] for sent in dev_getter.sentences()]\n",
        "dev_sent_ids = [[sent_id for token, label, corres_tok, sent_id in sent] for sent in dev_getter.sentences()]\n",
        "\n",
        "print(train_sentences[10])\n",
        "print(train_labels[10])\n",
        "\n",
        "print(len(dev_sentences))\n",
        "print(len(test_sentences))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['הם', 'התבקשו', 'לדווח', 'למשטרה', 'על', 'תנועותיהם', '.']\n",
            "['PRP', 'VB', 'VB', 'PREPOSITION^DEF^NN', 'IN', 'NN', 'yyDOT']\n",
            "490\n",
            "712\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2J4_RzLeGGov"
      },
      "source": [
        "### Put everything on CUDA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q3W99WhkCJ8I",
        "outputId": "a0463b1b-072d-4811-807c-ac3012d30c1d"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "n_gpu = torch.cuda.device_count()\n",
        "torch.cuda.set_device(0)\n",
        "\n",
        "print(\"Device: \" + str(device))\n",
        "print(\"Number of gpus: \" + str(n_gpu))\n",
        "print(\"Name of gpu: \" + torch.cuda.get_device_name(0))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Device: cuda\n",
            "Number of gpus: 4\n",
            "Name of gpu: GeForce RTX 2080 Ti\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u89uX2GYCJ8L"
      },
      "source": [
        "MAX_LEN = 150\n",
        "bs = 32"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8rtFukxAGMXQ"
      },
      "source": [
        "### Tokenize the training set (with BERT)\n",
        "See the example of how the tokenization looks like in the end"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rPjxTI1ACJ8O",
        "outputId": "32602c68-df9c-456c-ff3d-368d269a50ea"
      },
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased', do_lower_case=False)\n",
        "def tokenize(sentences, orig_labels):\n",
        "    tokenized_texts = []\n",
        "    labels = []\n",
        "    for sent, sent_labels in zip(sentences, orig_labels):\n",
        "        bert_tokens = []\n",
        "        bert_labels = []\n",
        "        for orig_token, orig_label in zip(sent, sent_labels):\n",
        "            b_tokens = tokenizer.tokenize(orig_token)\n",
        "            bert_tokens.extend(b_tokens)\n",
        "            for b_token in b_tokens:\n",
        "                bert_labels.append(orig_label)\n",
        "        tokenized_texts.append(bert_tokens)\n",
        "        labels.append(bert_labels)\n",
        "        assert len(bert_tokens) == len(bert_labels)\n",
        "    return tokenized_texts, labels\n",
        "\n",
        "train_tokenized_texts, train_tokenized_labels = tokenize(train_sentences, train_labels)\n",
        "print(train_tokenized_texts[10])\n",
        "print(train_tokenized_labels[10])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['הם', 'ה', '##ת', '##בק', '##שו', 'ל', '##דו', '##וח', 'ל', '##משטרה', 'על', 'ת', '##נוע', '##ות', '##יהם', '.']\n",
            "['PRP', 'VB', 'VB', 'VB', 'VB', 'VB', 'VB', 'VB', 'PREPOSITION^DEF^NN', 'PREPOSITION^DEF^NN', 'IN', 'NN', 'NN', 'NN', 'NN', 'yyDOT']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kL4xsjGfGaaz"
      },
      "source": [
        "### Get a list of all possible POS-tag labels\n",
        "Note that this list contains simple and multi POS tags. There are 50 Simple POS-tags (in isolation), and 315 POS-tags (combining simple and multi POS-tags). Making even more complex tags (for example, joining the 'feats' tag as well) results in an exponentially larger and more sparse label space.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jagl9jfGCJ8Q",
        "outputId": "6759eaed-240a-4c78-9363-d504520f4d67"
      },
      "source": [
        "data = train_df\n",
        "tag_vals = list(set(data[\"upostag\"].values))\n",
        "tags = ['PAD'] + tag_vals\n",
        "tag2idx = {tag:idx for idx, tag in enumerate(tags)}\n",
        "idx2tag = {idx:tag for idx, tag in enumerate(tags)}\n",
        "\n",
        "print(tag2idx)\n",
        "# print(idx2tag)\n",
        "print(len(tags))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'PAD': 0, 'REL^PREPOSITION^CDT': 1, 'DEF^DTT': 2, 'CONJ^VB^AT^PRP': 3, 'CONJ^yyQUOT^NNT': 4, 'PREPOSITION^ADVERB^CD': 5, 'TEMP^PRP': 6, 'CONJ^TEMP^RB': 7, 'AT': 8, 'CONJ^REL^COP': 9, 'PREPOSITION^ADVERB^NCD': 10, 'PREPOSITION^ADVERB^CDT': 11, 'CONJ^DTT': 12, 'TEMP^PREPOSITION^DEF^NN': 13, 'TEMP^NNP': 14, 'IN^IN^NNT': 15, 'DEF^P': 16, 'ADVERB^DTT': 17, 'VB^AT^S_ANP': 18, 'CONJ^DEF^NNP': 19, 'CD': 20, 'PREPOSITION^PREPOSITION^DEF^PRP': 21, 'PREPOSITION^yyQUOT^DEF^NN': 22, 'REL^DTT': 23, 'CONJ^IN': 24, 'POS': 25, 'PREPOSITION^JJ': 26, 'TEMP^NNT': 27, 'REL^yyQUOT^NNP': 28, 'CONJ^NNP': 29, 'IN^NN': 30, 'PREPOSITION^CDT': 31, 'PREPOSITION^NNP': 32, 'CONJ^CC': 33, 'yyLRB': 34, 'DEF^NN': 35, 'BNT': 36, 'REL^DEF^BN': 37, 'REL^yyQUOT^JJ': 38, 'PREPOSITION^DEF^yyQUOT^NNP': 39, 'REL': 40, 'NN': 41, 'IN^NNT': 42, 'TEMP^RB': 43, 'DEF^RB': 44, 'REL^JJ': 45, 'IN': 46, 'JJ': 47, 'PREPOSITION^DEF^CD': 48, 'CONJ^REL^PREPOSITION^NN': 49, 'CONJ^yyQUOT^NN': 50, 'CONJ^PREPOSITION^CDT': 51, 'CONJ^MD': 52, 'IN^PRP': 53, 'CONJ^yyQUOT^DEF^JJ': 54, 'PREPOSITION^POS^S_PRN': 55, 'PREPOSITION^CC': 56, 'CONJ^INTJ': 57, 'CONJ^PRP': 58, 'CONJ^REL^BN': 59, 'PREPOSITION^JJT': 60, 'REL^PREPOSITION^DTT': 61, 'PREPOSITION^DEF^BN': 62, 'IN^VB': 63, 'PREPOSITION^DEF': 64, 'REL^DT': 65, 'CONJ^yyQUOT^IN': 66, 'TEMP^VB': 67, 'CONJ^DEF^JJ': 68, 'CONJ^IN^DEF^NN': 69, 'PREPOSITION^ADVERB^NN': 70, 'REL^COP': 71, 'REL^BN': 72, 'PREPOSITION^yyQUOT^NN': 73, 'CONJ^IN^PRP': 74, 'CONJ^REL^NNT': 75, 'CONJ^PREPOSITION^NN': 76, 'yyCLN': 77, 'yyQM': 78, 'ZVL^PREPOSITION^NNT': 79, 'REL^MD': 80, 'CONJ^PREPOSITION^CC': 81, 'REL^AT': 82, 'ZVL^DEF^NNT': 83, 'PREPOSITIONIN^PREPOSITION^NN': 84, 'PREPOSITION^DEF^NN': 85, 'REL^DEF^JJ': 86, 'DT': 87, 'VB': 88, 'CONJ': 89, 'IN^IN': 90, 'PREPOSITION^IN': 91, 'CONJ^CDT': 92, 'PREPOSITION^REL^VB': 93, 'CONJ^REL^VB': 94, 'PREPOSITION^VB': 95, 'REL^CC': 96, 'ZVL^PREPOSITION^DEF^NN': 97, 'PREPOSITION^PREPOSITION^DEF^NN': 98, 'CONJ^COP': 99, 'REL^yyQUOT^COP': 100, 'IN^NCD': 101, 'CONJ^yyQUOT^NNP': 102, 'CONJ^PREPOSITION^DEF^NNP': 103, 'CONJ^DEF^CD': 104, 'PREPOSITION^PREPOSITION^DEF^DEF': 105, 'PREPOSITION^REL^COP': 106, 'AT^S_PRN': 107, 'CONJ^IN^S_PRN': 108, 'DEF^yyQUOT^JJ': 109, 'DEF^MD': 110, 'CONJ^QW': 111, 'yyDOT': 112, 'CONJ^RB': 113, 'CC': 114, 'REL^PREPOSITION^DEF^PRP': 115, 'REL^IN^PRP': 116, 'PREPOSITION^DEF^CDT': 117, 'IN^CDT': 118, 'PREPOSITION^DEF^RB': 119, 'REL^PREPOSITION^RB': 120, 'IN^RB': 121, 'CONJ^DT': 122, 'CONJ^TEMP^VB': 123, 'REL^yyQUOT^MD': 124, 'CONJ^PREPOSITION^PRP': 125, 'PREPOSITION^DEF^JJ': 126, 'NN^yyDOT': 127, 'REL^yyQUOT^PRP': 128, 'CONJ^PREPOSITION^NNP': 129, 'MD': 130, 'CONJ^PREPOSITION^DEF^CD': 131, 'CONJ^JJ': 132, 'CONJ^PREPOSITION^BN': 133, 'yySCLN': 134, 'DEF^JJ': 135, 'PREPOSITION^PREPOSITION^NN': 136, 'PREPOSITION^IN^DEF^PRP': 137, 'CONJ^DEF^NN': 138, 'CONJ^DEF^yyQUOT^NN': 139, 'REL^DEF^NN': 140, 'ZVL^PREPOSITION^NN': 141, 'REL^ADVERB^CD': 142, 'PREPOSITION^CD': 143, 'REL^yyQUOT^BN': 144, 'INTJ': 145, 'REL^PRP': 146, 'VB^AT^PRP': 147, 'REL^PREPOSITION^yyQUOT^NNP': 148, 'CC^ZVL^DEF^NN': 149, 'RB': 150, 'CONJ^PREPOSITION^RB': 151, 'yyDASH': 152, 'CONJ^REL^IN': 153, 'TEMP^DEF^BN': 154, 'DEF^CD': 155, 'ZVL^CD': 156, 'CONJ^PREPOSITION^CD': 157, 'DEF^COP': 158, 'PRP': 159, 'CONJ^AT': 160, 'VB^AT^S_PRN': 161, 'NNP': 162, 'CONJ^PREPOSITION^BNT': 163, 'P': 164, 'CONJ^BN': 165, 'PREPOSITION^yyQUOT^NNT': 166, 'yyEXCL': 167, 'JJT': 168, 'REL^VB': 169, 'CONJ^PREPOSITION^yyQUOT^CDT': 170, 'TEMP^DEF^NN': 171, 'REL^DEF^NNP': 172, 'CONJ^VB': 173, 'PREPOSITION^PRP': 174, 'DEF^BN^AT^PRP': 175, 'yyELPS': 176, 'P^NN': 177, 'REL^PREPOSITION^DT': 178, 'CONJ^REL^DEF^NN': 179, 'IN^S_PRN': 180, 'NCD': 181, 'PREPOSITION^DT': 182, 'DEF^PREPOSITION^NNT': 183, 'REL^CDT': 184, 'DEF': 185, 'REL^JJT': 186, 'TEMP^IN': 187, 'yyQUOT': 188, 'CONJ^IN^NNT': 189, 'QW': 190, 'POS^S_PRN': 191, 'CONJ^NN': 192, 'CONJ^REL^EX': 193, 'CONJ^PREPOSITION^P': 194, 'PREPOSITION^BN': 195, 'ZVL^DEF^NNP': 196, 'PREPOSITION^NNT': 197, 'ZVL^JJT': 198, 'IN^NNP': 199, 'REL^PREPOSITION^BN': 200, 'CONJ^yyQUOT^DEF^NN': 201, 'PREPOSITION^DEF^P': 202, 'TEMP^COP': 203, 'REL^AT^S_PRN': 204, 'PREPOSITION^yyQUOT^NNP': 205, 'REL^P': 206, 'PREPOSITION^yyQUOT^PREPOSITION^NNT': 207, 'ZVL^ZVL': 208, 'REL^PREPOSITION^NNP': 209, 'TEMP^PREPOSITION^NNP': 210, 'NEG': 211, 'CONJ^PREPOSITION^DTT': 212, 'REL^yyQUOT^PREPOSITION^NN': 213, 'REL^DEF^CD': 214, 'CONJ^NNT': 215, 'PREPOSITION^RB^S_PRN': 216, 'DEF^NNP': 217, 'CONJ^POS': 218, 'REL^yyQUOT^DEF^NN': 219, 'PREPOSITION^ PREPOSITION^DEF^NN': 220, 'DEF^NCD': 221, 'REL^NNT': 222, 'PREPOSITION^DEF^NNT': 223, 'IN^REL^NNT': 224, 'ZVL^RB': 225, 'BN': 226, 'PREPOSITION^NN': 227, 'REL^PREPOSITION^NN': 228, 'TEMP^PREPOSITION^NN': 229, 'ZVL^NNT': 230, 'TEMP^NN': 231, 'PREPOSITION^BNT': 232, 'CDT': 233, 'DEF^yyQUOT^NNP': 234, 'PREPOSITION^yyQUOT^PREPOSITION^DEF^NN': 235, 'PREPOSITION^IN^S_PRN': 236, 'ZVL^DEF^NN': 237, 'CONJ^EX': 238, 'DEF^DEF^NN': 239, 'CONJ^IN^JJT': 240, 'NNT': 241, 'DTT': 242, 'IN^yyQUOT^VB': 243, 'REL^RB': 244, 'EX': 245, 'CONJ^PREPOSITION^QW': 246, 'ADVERB^NCD': 247, 'TEMP^BN': 248, 'CONJ^JJT': 249, 'CONJ^PREPOSITION^yyQUOT^NNP': 250, 'IN^DEF^NN': 251, 'CONJ^CD': 252, 'REL^CD': 253, 'DEF^NNT': 254, 'CONJ^BNT': 255, 'REL^PREPOSITION^NNT': 256, 'PREPOSITION^POS': 257, 'ZVL': 258, 'PREPOSITION^NEG': 259, 'PREPOSITION^yyQUOT^BN': 260, 'REL^QW': 261, 'CONJ^DEF^DTT': 262, 'REL^VB^AT^PRP': 263, 'CONJ^DEF^P': 264, 'ZVL^IN': 265, 'PREPOSITIONIN^NN': 266, 'REL^PREPOSITION^DEF^NN': 267, 'PREPOSITION^DEF^PRP': 268, 'PREPOSITION^NCD': 269, 'IN^JJT': 270, 'yyCM': 271, 'REL^IN': 272, 'REL^IN^S_PRN': 273, 'CONJ^DEF^MD': 274, 'REL^yyQUOT^RB': 275, 'REL^yyQUOT^VB': 276, 'PREPOSITION^PREPOSITION^NNT': 277, 'CONJ^yyQUOT^VB': 278, 'CONJ^REL^NN': 279, 'DEF^yyQUOT^NN': 280, 'REL^NN': 281, 'PREPOSITION': 282, 'COP': 283, 'REL^EX': 284, 'AT^PRP': 285, 'TEMP^PREPOSITION^PRP': 286, 'CONJ^PREPOSITION^DEF^NN': 287, 'IN^RB^CD': 288, 'REL^NNP': 289, 'CONJ^BN^AT^PRP': 290, 'ADVERB^NN': 291, 'CONJ^TEMP^BN': 292, 'CONJ^PREPOSITION^NNT': 293, 'CONJ^DEF^BN': 294, 'PREPOSITION^RB': 295, 'DEF^BN': 296, 'REL^yyQUOT^NN': 297, 'PREPOSITION^DEF^NNP': 298, 'CONJ^P': 299, 'ZVL^NNP': 300, 'IN^DTT': 301, 'ADVERB^CD': 302, 'CONJ^PREPOSITION^JJ': 303, 'ZVL^COP': 304, 'CONJ^PREPOSITION^DEF^JJ': 305, 'CONJ^PREPOSITION^DEF^PRP': 306, 'PREPOSITION^P': 307, 'RB^S_PRN': 308, 'PREPOSITION^QW': 309, 'ZVL^NN': 310, 'DEF^PRP': 311, 'PREPOSITION^DTT': 312, 'yyRRB': 313, 'TEMP^DEF^CD': 314}\n",
            "315\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3v-mQybaHPhg"
      },
      "source": [
        "### All the technical stuff\n",
        "To make sentences and labels the same length and in tensor form"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZcJSokbcCJ8S"
      },
      "source": [
        "def pad_sentences_and_labels(tokenized_texts, labels):\n",
        "    input_ids = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts],\n",
        "                              maxlen = MAX_LEN, dtype = \"float32\", truncating = \"post\", padding = \"post\", value = tag2idx['PAD'])\n",
        "    tags = pad_sequences([[tag2idx.get(l) for l in lab] for lab in labels], \n",
        "                         maxlen = MAX_LEN, value = tag2idx['PAD'], padding = \"post\",\n",
        "                        dtype = \"float32\", truncating = \"post\")\n",
        "    attention_masks = [[float(i>0) for i in ii] for ii in input_ids]\n",
        "    return input_ids, tags, attention_masks\n",
        "\n",
        "input_ids, tags, attention_masks = pad_sentences_and_labels(train_tokenized_texts, train_tokenized_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nWUcw1MTCJ8U"
      },
      "source": [
        "tr_inputs = torch.tensor(input_ids, dtype=torch.long)\n",
        "tr_tags = torch.tensor(tags, dtype=torch.long)\n",
        "tr_masks = torch.tensor(attention_masks, dtype=torch.long)\n",
        "\n",
        "train_data = TensorDataset(tr_inputs, tr_masks, tr_tags)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler = train_sampler, batch_size=bs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5RYoAIQRHgv2"
      },
      "source": [
        "### Performing the Fine-tuning\n",
        "As you can see we didn't perform any hyperparameter search. This is intended. Of course, HP-optimization can make the model better (you are welcome to try it yourself), but we didn't want to make the best model, we wanted to investigate how well contextualized models can represent complex morphology, so all of our models use the same fine-tuning procedure described below, and the only thing that changes is the label that each word-piece receives (described in detail in the theoratical post on this paper)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "bznq36qeCJ8W",
        "outputId": "5b81903d-fc24-4b6f-b1de-c4fd1eac7e9e"
      },
      "source": [
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "model = BertForTokenClassification.from_pretrained('bert-base-multilingual-cased',\n",
        "                                                   num_labels=len(tag2idx),\n",
        "                                                   output_attentions = False,\n",
        "                                                   output_hidden_states = False)\n",
        "model.cuda()\n",
        "FULL_FINETUNING = True\n",
        "if FULL_FINETUNING:\n",
        "    param_optimizer = list(model.named_parameters())\n",
        "    no_decay = ['bias', 'gamma', 'beta']\n",
        "    optimizer_grouped_parameters = [\n",
        "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
        "         'weight_decay_rate': 0.01},\n",
        "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
        "         'weight_decay_rate': 0.0}\n",
        "    ]\n",
        "else:\n",
        "    param_optimizer = list(model.classifier.named_parameters())\n",
        "    optimizer_grouped_parameters = [{\"params\": [p for n, p in param_optimizer]}]\n",
        "\n",
        "optimizer = AdamW(optimizer_grouped_parameters, lr=3e-5, eps=1e-8)\n",
        "\n",
        "from seqeval.metrics import f1_score\n",
        "\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=2).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "#     print (pred_flat, labels_flat)\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
        "\n",
        "epochs = 15\n",
        "max_grad_norm = 1.0\n",
        "\n",
        "# Total number of training steps is number of batches * number of epochs.\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "# Create the learning rate scheduler.\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps=0,\n",
        "                                            num_training_steps=total_steps)\n",
        "\n",
        "## Store the average loss after each epoch so we can plot them.\n",
        "loss_values, validation_loss_values = [], []\n",
        "for _ in trange(epochs, desc=\"Epoch\"):\n",
        "    # TRAIN loop\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "        # add batch to gpu\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "        model.zero_grad()\n",
        "        # forward pass\n",
        "        outputs = model(b_input_ids, token_type_ids=None,\n",
        "                     attention_mask=b_input_mask, labels=b_labels)\n",
        "        # get the loss\n",
        "        loss = outputs[0]\n",
        "        # Perform a backward pass to calculate the gradients.\n",
        "        loss.backward()\n",
        "        # track train loss\n",
        "        total_loss += loss.item() \n",
        "        # Clip the norm of the gradient\n",
        "        # This is to help prevent the \"exploding gradients\" problem.\n",
        "        torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=max_grad_norm)\n",
        "        # update parameters\n",
        "        optimizer.step()\n",
        "        # Update the learning rate.\n",
        "        scheduler.step()\n",
        "        \n",
        "    # Calculate the average loss over the training data.\n",
        "    avg_train_loss = total_loss / len(train_dataloader)\n",
        "    print(\"Average train loss: {}\".format(avg_train_loss))\n",
        "    \n",
        "    # Store the loss value for plotting the learning curve.\n",
        "    loss_values.append(avg_train_loss)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch:   7%|▋         | 1/15 [00:58<13:45, 58.94s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Average train loss: 2.2720749048810256\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  13%|█▎        | 2/15 [02:02<13:05, 60.43s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Average train loss: 0.821627008679666\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  20%|██        | 3/15 [03:12<12:36, 63.06s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Average train loss: 0.5413947211284387\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  27%|██▋       | 4/15 [04:27<12:16, 66.92s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Average train loss: 0.39871315207136304\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  33%|███▎      | 5/15 [05:46<11:42, 70.26s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Average train loss: 0.31264687878520864\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  40%|████      | 6/15 [07:04<10:54, 72.67s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Average train loss: 0.25154486896568223\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  47%|████▋     | 7/15 [08:23<09:56, 74.55s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Average train loss: 0.20845407497529922\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  53%|█████▎    | 8/15 [09:42<08:50, 75.82s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Average train loss: 0.17870022454544118\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  60%|██████    | 9/15 [11:02<07:43, 77.18s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Average train loss: 0.15089197540165564\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  67%|██████▋   | 10/15 [12:25<06:34, 78.94s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Average train loss: 0.13223850584932065\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  73%|███████▎  | 11/15 [13:46<05:17, 79.48s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Average train loss: 0.11638171909573047\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  80%|████████  | 12/15 [15:05<03:58, 79.42s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Average train loss: 0.10619803432277158\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  87%|████████▋ | 13/15 [16:26<02:39, 79.99s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Average train loss: 0.09557023781694864\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  93%|█████████▎| 14/15 [17:48<01:20, 80.63s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Average train loss: 0.09014363510926303\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 100%|██████████| 15/15 [19:06<00:00, 76.45s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Average train loss: 0.08390635945589135\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Snf429k5IjcA"
      },
      "source": [
        "### More Hebrew Fun\n",
        "Remember that the model predicts a POS tag (simple or multi, one of the 315 from above) for each word-piece. But we don't want to evaluate word-pieces, we want to evaluate on whole words, because words can have multi-tags and also because we want to make a fair comparison with our baseline model that uses the actual morphemes (see example in the paper) - therefore all of our models are evaluated on word-level (as opposed to morpheme level, for example).\n",
        "\n",
        "This setting of the aggregation of the wordpieces (multi-)tags to a whole word assumes we combine the (multi-)tag of each wordpiece to a single multi-tag. It can easily be adjusted for the case where we take the first (multi-)tag to be the tag for the whole word."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Z_M26oGCJ8Y"
      },
      "source": [
        "# Function receives a sentence with its labels, and the tokenized sentence and labels\n",
        "def aggr_toks_labels_tags(orig_words, orig_labels, tok_wordps, tok_labels, predicted_tags):\n",
        "    \n",
        "    joint_tokens = []\n",
        "    joint_labels = []\n",
        "    joint_predicted = []\n",
        "    \n",
        "    for word in orig_words:\n",
        "        aggregated_tokenized = \"\"\n",
        "        aggregated_label = \"\"\n",
        "        aggregated_predicted = \"\"\n",
        "        aggregated_test = \"\"\n",
        "        \n",
        "        while aggregated_tokenized != word:\n",
        "            tmpTok = tok_wordps.pop(0)\n",
        "            if tmpTok.startswith(\"##\"):\n",
        "                tmpTok = tmpTok[2:]\n",
        "                \n",
        "            tmpLab = tok_labels.pop(0)\n",
        "            aggregated_label += '^'\n",
        "            aggregated_label += tmpLab\n",
        "\n",
        "            tmpPred = predicted_tags.pop(0)\n",
        "\n",
        "            aggregated_predicted += '^'\n",
        "            aggregated_predicted += tmpPred\n",
        "                \n",
        "            aggregated_tokenized += tmpTok\n",
        "\n",
        "            \n",
        "        joint_tokens.append(aggregated_tokenized)\n",
        "        joint_labels.append(aggregated_label)\n",
        "        joint_predicted.append(aggregated_predicted)\n",
        "\n",
        "        \n",
        "    assert len(joint_tokens) == len(orig_words)\n",
        "    assert len(joint_tokens) == len(joint_predicted)\n",
        "    return joint_tokens, joint_labels, joint_predicted"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FthB9-FNK0oJ"
      },
      "source": [
        "### Building the evaluation models and evaluating on the dev set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4yOTrhT6CJ8a"
      },
      "source": [
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=2).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
        "\n",
        "def delete_pads_from_preds(predicted_tags, test_tags):\n",
        "    clean_predicted = []\n",
        "    clean_test = []\n",
        "    \n",
        "    for ix in range(0, len(test_tags)):\n",
        "        if test_tags[ix] != 'PAD':\n",
        "            clean_predicted.append(predicted_tags[ix])\n",
        "            clean_test.append(test_tags[ix])\n",
        "            \n",
        "    return clean_predicted, clean_test\n",
        "    \n",
        "def calculate_accuracy(df):\n",
        "    numOfCorrectPredictions = 0\n",
        "    for index in df.index:\n",
        "        orig_pos = df.at[index, 'orig_label']\n",
        "        pred_pos = df.at[index, 'predicted_tag']\n",
        "        if orig_pos == pred_pos:\n",
        "            numOfCorrectPredictions += 1\n",
        "    return numOfCorrectPredictions/len(df)\n",
        "                \n",
        "def test_model(sentence, labels, tok_sent, tok_labels, corres_tokens, sent_id):\n",
        "    input_ids, tags, attention_masks = pad_sentences_and_labels([tok_sent], [tok_labels])\n",
        "\n",
        "    val_inputs = torch.tensor(input_ids, dtype=torch.long)\n",
        "    val_tags = torch.tensor(tags, dtype=torch.long)\n",
        "    val_masks = torch.tensor(attention_masks, dtype=torch.long)\n",
        "\n",
        "    test_data = TensorDataset(val_inputs, val_masks, val_tags)\n",
        "    test_sampler = SequentialSampler(test_data)\n",
        "    test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=bs)\n",
        "\n",
        "    model.eval()\n",
        "    eval_loss, eval_accuracy = 0, 0\n",
        "    nb_eval_steps, nb_eval_examples = 0, 0\n",
        "    predictions, true_labels = [], []\n",
        "    counter = 0\n",
        "    for batch in test_dataloader:\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(b_input_ids, token_type_ids=None,\n",
        "                                attention_mask=b_input_mask, labels=b_labels)\n",
        "        logits = outputs[1].detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "        predictions.append([list(p) for p in np.argmax(logits, axis=2)])\n",
        "        \n",
        "        true_labels.append(label_ids)\n",
        "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
        "\n",
        "        eval_loss += outputs[0].mean().item()\n",
        "        eval_accuracy += flat_accuracy(logits, label_ids)\n",
        "\n",
        "        nb_eval_examples += b_input_ids.size(0)\n",
        "        nb_eval_steps += 1\n",
        "    eval_loss = eval_loss / nb_eval_steps\n",
        "    \n",
        "    pred_tags = [idx2tag[p_ii] for p in predictions for p_i in p for p_ii in p_i]\n",
        "    joint_tokenized, joint_labels, preds = aggr_toks_labels_tags(sentence, labels, tok_sent, tok_labels, \n",
        "                                                                        pred_tags)\n",
        "    \n",
        "    tmp = {'word': sentence, 'orig_label': labels, 'predicted_tag': preds, \n",
        "           'corresToken': corres_tokens, 'sent_id': sent_id}\n",
        "    tmp_df = pd.DataFrame(data=tmp)\n",
        "    return tmp_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DoZrbpn4CJ8c"
      },
      "source": [
        "full_dev_df = pd.DataFrame()\n",
        "dev_tokenized_texts, dev_tokenized_labels = tokenize(dev_sentences, dev_labels)\n",
        "for sent, label, tok_sent, tok_label, corresTokens, sent_id in zip(dev_sentences, dev_labels, dev_tokenized_texts, \n",
        "                                                                   dev_tokenized_labels, dev_corresTokens, \n",
        "                                                                   dev_sent_ids):\n",
        "    eval_df = test_model(sent, label, tok_sent, tok_label, corresTokens, sent_id)\n",
        "    full_dev_df = full_dev_df.append(eval_df, ignore_index=True, sort=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QYzY-Q8nLMNA"
      },
      "source": [
        "### How the dev DataFrame looks like after evaluation\n",
        "Under the 'predicted_tag' column we can see the aggragated tag. Some processing is still required for the predicted tag to be comparable to the 'orig_label' tag - see the next cells.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qO_BRxRtCJ8f",
        "outputId": "7248f975-ed63-4283-8ace-523b5dda7d52"
      },
      "source": [
        "full_dev_df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>word</th>\n",
              "      <th>orig_label</th>\n",
              "      <th>predicted_tag</th>\n",
              "      <th>corresToken</th>\n",
              "      <th>sent_id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>עשרות</td>\n",
              "      <td>CDT</td>\n",
              "      <td>^CD</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>אנשים</td>\n",
              "      <td>NN</td>\n",
              "      <td>^NN</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>מגיעים</td>\n",
              "      <td>BN</td>\n",
              "      <td>^BN^BN</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>מתאילנד</td>\n",
              "      <td>PREPOSITION^NNP</td>\n",
              "      <td>^PREPOSITION^NNP^PREPOSITION^NNP^PREPOSITION^NNP</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>לישראל</td>\n",
              "      <td>PREPOSITION^NNP</td>\n",
              "      <td>^PREPOSITION^NNP</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      word       orig_label                                     predicted_tag  \\\n",
              "0    עשרות              CDT                                               ^CD   \n",
              "1    אנשים               NN                                               ^NN   \n",
              "2   מגיעים               BN                                            ^BN^BN   \n",
              "3  מתאילנד  PREPOSITION^NNP  ^PREPOSITION^NNP^PREPOSITION^NNP^PREPOSITION^NNP   \n",
              "4   לישראל  PREPOSITION^NNP                                  ^PREPOSITION^NNP   \n",
              "\n",
              "   corresToken  sent_id  \n",
              "0            1        1  \n",
              "1            2        1  \n",
              "2            3        1  \n",
              "3            4        1  \n",
              "4            5        1  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r2iSfdqjM1lr"
      },
      "source": [
        "### Splitting the predicted tag\n",
        "As mentioned in the theoratical post and in the paper, we split the predicted tag to that it includes only unique tags by the order they were predicted. We make a list of tags from both the original tag and the predicted (uniquely filtered) tag, and those lists will be compared."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UwCBw660CJ8q"
      },
      "source": [
        "from more_itertools import unique_everseen\n",
        "\n",
        "def unique_vals_to_list(df):\n",
        "    for index in df.index:\n",
        "        joint_pred = df.at[index, 'predicted_tag']\n",
        "        joint_orig = df.at[index, 'orig_label']\n",
        "        \n",
        "        predicted_tag_list = joint_pred.split('^')\n",
        "        predicted_tag_list_no_empty = list(filter(None, predicted_tag_list))\n",
        "        original_tag_list = joint_orig.split('^')\n",
        "        original_tag_list_no_empty = list(filter(None, original_tag_list))\n",
        "\n",
        "        \n",
        "        df.at[index, 'predicted_tag'] = list(unique_everseen(predicted_tag_list_no_empty))\n",
        "        df.at[index, 'orig_label'] = list(unique_everseen(original_tag_list_no_empty))\n",
        "        \n",
        "        \n",
        "unique_vals_to_list(full_dev_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3JVpfCpuNhhP"
      },
      "source": [
        "### Comparing predicted vs. original label\n",
        "Two metrics of evaluation are available - exact match and existence.\n",
        "Exact match - pretty straight forward, an all-or-nothing approach, if the original label is IN^DEF^NN and the model predicted exactly that - great, if it predicted IN^NN - it doesn't count as success. This is a tough test, sure. What if the model actually predicted the tags but not in the right order (suppose DEF^IN^NN) or maybe just got 2 tags out of 3? doesn't that count for something?\n",
        "Sure thing. So the second metric is the existence, where we calculate the standard precision and recall for the tags that appeared in both the predicted and original tags and report the F1 score based on those calculations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HzYYluPhCJ8u"
      },
      "source": [
        "def exact_match_accuracy(df):\n",
        "    exact_matches = 0\n",
        "    for index in df.index:\n",
        "        if df.at[index, 'orig_label'] == df.at[index, 'predicted_tag']:\n",
        "            exact_matches += 1\n",
        "            \n",
        "    return exact_matches\n",
        "\n",
        "print(\"DEV - Exact Match Accuracy = {0:.2f}%\".format(exact_match_accuracy(dev_combined)/len(dev_combined) * 100))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iq2PRcP9CJ8w"
      },
      "source": [
        "def existence_accuracy(df):\n",
        "    # correct tag = appeared in predicted and in original\n",
        "    total_orig_num_of_labels = 0\n",
        "    total_predicted_num_of_labels = 0\n",
        "    total_num_of_correct_tags = 0\n",
        "    \n",
        "    for index in df.index:\n",
        "        orig_list = df.at[index, 'orig_label']\n",
        "        predicted_list = df.at[index, 'predicted_tag']\n",
        "        total_orig_num_of_labels += len(orig_list)\n",
        "        total_predicted_num_of_labels += len(predicted_list)\n",
        "        total_num_of_correct_tags += len(set(orig_list).intersection(set(predicted_list)))\n",
        "        \n",
        "    precision = total_num_of_correct_tags / total_predicted_num_of_labels * 100\n",
        "    recall = total_num_of_correct_tags / total_orig_num_of_labels * 100\n",
        "    f1 = 2*precision*recall/(precision+recall)\n",
        "    \n",
        "    print(\"Precision: {0:.2f}%\".format(precision))\n",
        "    print(\"Recall: {0:.2f}%\".format(recall))\n",
        "    print(\"F1: {0:.2f}%\".format(f1))\n",
        "    \n",
        "print(\"DEV:\")\n",
        "existence_accuracy(dev_combined)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jWNHrYf_PE-I"
      },
      "source": [
        "### That's it!\n",
        "Some more things that can be done (and maybe will be done by me in the future...)\n",
        "\n",
        "\n",
        "1.   Fine-tune on ordered sentences, that is, see if the learning improves if we apply some order to the sentences (for example arrange the sentences from shortest to longest). The idea behind this is that shorter sentences probably have a simpler syntactic structure, and fine-tuning on them first will hhelp the model learn basic and fundamental properties of Hebrew structure.\n",
        "2.   Change the tokenizer - I think this is the biggest issue in processing Hebrew in general. It was demonstrated in the paper that BERT's tokenizer is not suitable for languages with complex morphology - we should probably use a different tokenizer for such languages (but that would also require pre-training BERT all together).\n",
        "\n"
      ]
    }
  ]
}
