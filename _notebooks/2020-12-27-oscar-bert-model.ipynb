{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2020-12-27-oscar-bert-model.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yBpt9AfNLcRq"
      },
      "source": [
        "# HeBERT vs. mBERT\r\n",
        "> \"Testing the new model for Hebrew on cPOS-tagging\"\r\n",
        "\r\n",
        "- toc:false\n",
        "- branch: master\n",
        "- badges: true\n",
        "- comments: true\n",
        "- author: Stav Klein\n",
        "- categories: [fastpages, jupyter]\n",
        "- image: images/oscar-bert.png"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5_iBXqrGAyPX"
      },
      "source": [
        "#hide\r\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bnuOgCcKNIaj"
      },
      "source": [
        "### Data Overview"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G8i_30I_MODh"
      },
      "source": [
        "Avichay Chriqui and Dr. Inbal Yahav Shenberger recently released a new model for Hebrew based on BERT's architecture and trained on the OSCAR corpus. The model is built with the huggingface library and uses the library's `AutoTokenizer` (which is WordPieces since the model is BERT-based) and the model itself is `AutoModelForMaskedLM`. \r\n",
        "Key differences between HeBERT and mBERT are summarized in the table below:\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        },
        "id": "XT39x1v1_iz-",
        "outputId": "76c39aad-3640-4e21-c29c-e11649b5d334"
      },
      "source": [
        "#hide_input\r\n",
        "models = {'HeBERT':['~ 30K', 'Hebrew Wiki, Hebrew OSCAR, User-generated content'], \r\n",
        "        'mBERT':['~ 2K', 'Hebrew Wiki']} \r\n",
        "  \r\n",
        "# Creates pandas DataFrame. \r\n",
        "models_df = pd.DataFrame(models, index =['# of word-pieces', \r\n",
        "                                'Training Data'])\r\n",
        "\r\n",
        "models_df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>HeBERT</th>\n",
              "      <th>mBERT</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th># of word-pieces</th>\n",
              "      <td>~ 30K</td>\n",
              "      <td>~ 2K</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Training Data</th>\n",
              "      <td>Hebrew Wiki, Hebrew OSCAR, User-generated content</td>\n",
              "      <td>Hebrew Wiki</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                             HeBERT        mBERT\n",
              "# of word-pieces                                              ~ 30K         ~ 2K\n",
              "Training Data     Hebrew Wiki, Hebrew OSCAR, User-generated content  Hebrew Wiki"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "id": "cW42i-QjG2Hv",
        "outputId": "79a81432-ba49-4ef0-b3ef-aaea54d61e29"
      },
      "source": [
        "#hide_input\r\n",
        "data = {'Size':['650MB', '9.8GB' , '150MB'], \r\n",
        "        '# of sentences':['3.8M', '20.8M', '350K'],} \r\n",
        "  \r\n",
        "# Creates pandas DataFrame. \r\n",
        "data_df = pd.DataFrame(data, index =['Hebrew Wiki', \r\n",
        "                                'Hebrew OSCAR',\r\n",
        "                                'UGC'])\r\n",
        "\r\n",
        "data_df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Size</th>\n",
              "      <th># of sentences</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Hebrew Wiki</th>\n",
              "      <td>650MB</td>\n",
              "      <td>3.8M</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Hebrew OSCAR</th>\n",
              "      <td>9.8GB</td>\n",
              "      <td>20.8M</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>UGC</th>\n",
              "      <td>150MB</td>\n",
              "      <td>350K</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               Size # of sentences\n",
              "Hebrew Wiki   650MB           3.8M\n",
              "Hebrew OSCAR  9.8GB          20.8M\n",
              "UGC           150MB           350K"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CAEKN2ChAu_r"
      },
      "source": [
        "It's clear from both tables that the HeBERT is order of magnitude larger than the Hebrew part of the multilingual BERT. I reproduced two experimental settings from this [paper from 2020 SIGMORPHON](https://www.aclweb.org/anthology/2020.sigmorphon-1.24/) (and see also the corresponding [blog post](https://stavkl.github.io/linguistics-for-nlp/fastpages/jupyter/2020/09/21/getting-the-life.html) for more details) in order to examine the contribution of huge amounts of data to the complex-POS tagging task. Both experiments ran in exactly the same settings, with the only differences being the choice of model and tokenizer.<br>\r\n",
        "Below are examples for the tokenization differences between models:<br>\r\n",
        "**mBERT tokenization**<br>\r\n",
        "![](https://github.com/stavkl/linguistics-for-nlp/raw/master/images/tokenized-examples/mbert-tokenized.PNG)<br>\r\n",
        "**HeBERT tokenization**<br>\r\n",
        "![](https://github.com/stavkl/linguistics-for-nlp/raw/master/images/tokenized-examples/hebert-tokenized.PNG)<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZjXKYC0ON0C"
      },
      "source": [
        "### Experiment 1: Word-level Multitag"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        },
        "id": "3rRQh9zbAprN",
        "outputId": "11513c35-26ae-4d55-fbbc-8c6748c5463b"
      },
      "source": [
        "#hide_input\r\n",
        "exp1 = {'HeBERT':['94.63', '96.13'], \r\n",
        "        'mBERT':['92.45', '94.09'],} \r\n",
        "  \r\n",
        "# Creates pandas DataFrame. \r\n",
        "exp1_df = pd.DataFrame(exp1, index =['Exact Match', \r\n",
        "                                'F1'])\r\n",
        "\r\n",
        "exp1_df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>HeBERT</th>\n",
              "      <th>mBERT</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Exact Match</th>\n",
              "      <td>94.63</td>\n",
              "      <td>92.45</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>F1</th>\n",
              "      <td>96.13</td>\n",
              "      <td>94.09</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "            HeBERT  mBERT\n",
              "Exact Match  94.63  92.45\n",
              "F1           96.13  94.09"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3u9EC8JmPq-8"
      },
      "source": [
        "This setting shows an improvement in both exact-match accuracy and existence-f1 measures. It indicates that for settings where the access to the inner structure is not crucial it might be better to use a larger model. Nevertheless, the results received for HeBERT here are still on par with models like [YAP](https://https://www.aclweb.org/anthology/Q19-1003.pdf) (and needless to say, YAP is actually tested on a harder problem, so it's not really a like-for-like comparison)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PR7sgyDzRY_c"
      },
      "source": [
        "### Experiment 2: (Multi)-tag per Wordpiece\r\n",
        "Recall that in this setting each wordpiece can receive a possibly different tag or multi-tag. The results show that for a significantly larger model the procedure of assigning each wordpiece a different tag eventually deteriorates performance, probably because this procedure is based on heuristics about Hebrew morphology and also because it was designed with many wordpieces in mind while in HeBERT many words don't break into wordpieces."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        },
        "id": "zjxUb2vvPoMO",
        "outputId": "e7569b7e-4db9-432a-8c02-f95677956cf1"
      },
      "source": [
        "#hide_input\r\n",
        "exp2 = {'HeBERT':['84.86', '84.62'], \r\n",
        "        'mBERT':['86.66', '88.71'],} \r\n",
        "  \r\n",
        "# Creates pandas DataFrame. \r\n",
        "exp2_df = pd.DataFrame(exp2, index =['Exact Match', \r\n",
        "                                'F1'])\r\n",
        "\r\n",
        "exp2_df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>HeBERT</th>\n",
              "      <th>mBERT</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Exact Match</th>\n",
              "      <td>84.86</td>\n",
              "      <td>86.66</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>F1</th>\n",
              "      <td>84.62</td>\n",
              "      <td>88.71</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "            HeBERT  mBERT\n",
              "Exact Match  84.86  86.66\n",
              "F1           84.62  88.71"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wUazCg0WInIJ"
      },
      "source": [
        "### My linguistic take on things: Using huge models for Hebrew is like solving a 100-piece puzzle using 30,000 pieces\r\n",
        "\r\n",
        "The results signify that for tasks that don't require specific knowledge about internal structure it might be better to use a significantly larger model like HeBERT. However, this improvement is not big and it seems possible to achive that much improvement by taking other measures on the original mBERT (and avoid the huge training cost altogether). Here are some ideas:\r\n",
        "\r\n",
        "1.   Change the tokenizer - I think that's the single most meaningful thing that can be done for processing Hebrew (and other semitic languages). It was already shown that wordpieces are useless for modelling complex morphology.\r\n",
        "2.   Also related to that - I hypothesize that Hebrew models can use a much smaller vocabulary. Hebrew is really more like a 100-piece puzzle in the sense that many morphemes have a well-defined purpose and they connect to the other morphemes in very specific ways. Having 30K pieces actually makes the puzzle much harder to solve. Improving the model's \"knowledge\" of the affixation processes can reduce training costs and might lead to better morphological disambiguation and so to other improvements down the pipeline.\r\n",
        "3. Imposing some structure on the vocabulary, instead of just going over a huge list looking for the largest wordpiece we can fit. This would require some rule-based intervention with the process but might be worth checking.\r\n",
        "4. Changing the task - LM predicts the next word in a sentence, which works great for English since English sentences exhibit a strict word order, and so by learning what the next word is we also encode syntactic roles. This is not the case for Hebrew as word-order can vary a lot. However, the internal structure of a Hebrew word does NOT vary a lot, so we can try transferring \"next-word prediction\" to the character level to capture more morphological knowledge (though it would not encode the non-concatenative template, that remains a problem).\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RXnHNL5krTq6"
      },
      "source": [
        "That's all for now,<br>\r\n",
        "until next time<br>\r\n",
        "Stav"
      ]
    }
  ]
}
