<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.0.0">Jekyll</generator><link href="https://stavkl.github.io/linguistics-for-nlp/feed.xml" rel="self" type="application/atom+xml" /><link href="https://stavkl.github.io/linguistics-for-nlp/" rel="alternate" type="text/html" /><updated>2020-09-26T08:07:50-05:00</updated><id>https://stavkl.github.io/linguistics-for-nlp/feed.xml</id><title type="html">Linguistics for NLP</title><subtitle>My linguistic perspective on NLP research.</subtitle><entry><title type="html">Getting the ###life out of Living: How adequate are word-pieces for modelling complex morphology?</title><link href="https://stavkl.github.io/linguistics-for-nlp/fastpages/jupyter/2020/09/21/getting-the-life.html" rel="alternate" type="text/html" title="Getting the ###life out of Living: How adequate are word-pieces for modelling complex morphology?" /><published>2020-09-21T00:00:00-05:00</published><updated>2020-09-21T00:00:00-05:00</updated><id>https://stavkl.github.io/linguistics-for-nlp/fastpages/jupyter/2020/09/21/getting-the-life</id><content type="html" xml:base="https://stavkl.github.io/linguistics-for-nlp/fastpages/jupyter/2020/09/21/getting-the-life.html">&lt;p&gt;&lt;em&gt;This post reviews the paper I presented at SIGMORPHON this year (see link in the publications section), that should have been presented at the ISCOL conference too but eventually didn’t.&lt;/em&gt;&lt;/p&gt;

&lt;hr /&gt;
&lt;h3 id=&quot;how-adequate-are-word-pieces-for-modelling-complex-morphology&quot;&gt;How adequate are word-pieces for modelling complex morphology?&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;tldr;&lt;/strong&gt; &lt;br /&gt;
not at all.&lt;/p&gt;

&lt;p&gt;The task: Part-of-speech Tagging&lt;br /&gt;
The language: Hebrew&lt;br /&gt;
The data: &lt;a href=&quot;https://github.com/OnlpLab/Hebrew_UD&quot;&gt;The Hebrew treebank&lt;/a&gt;&lt;br /&gt;
The means: fine-tuning Multilingual BERT&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;the-challenges-with-pos-tagging-in-hebrew&quot;&gt;The challenges with POS-tagging in Hebrew&lt;/h3&gt;
&lt;p&gt;This section only deals with the challenges in POS-tagging Hebrew, and in another post I’ll review the challenges of processing languages with non-concatenative morphology in general.
In NLP, a word is a space-delimited sequence of characters. Each word is composed of at least one &lt;strong&gt;morpheme&lt;/strong&gt;. A morpheme is the smallest unit of meaning, and it comes in two shapes: bound and free. A bound morpheme has to be part of a word, like the English plural suffix &lt;em&gt;-s&lt;/em&gt;, while a free morpheme can stand on its own, like the coordinator &lt;em&gt;and&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;The main difference between English and Hebrew in this sense is that Hebrew has a much higher morphemes-to-word ratio than English, so for example the sequence &lt;em&gt;and when I saw&lt;/em&gt; (4 free morphemes in English) is expressed in Hebrew using a single word - וכשראיתי /ve-kshe-raiti/ (and - bound, when - bound, saw.1st per.sg - free). Each morpheme in the Hebrew word has a different POS tag, which one should we choose? Easy, we don’t, we take them all (otherwise we loose valuable syntactic information that we do encode for English) - Introducing the &lt;strong&gt;multi-tag&lt;/strong&gt;, which is a POS-tag composed of POS-tags. In the example the multi-tag would be CONJ^REL\^VB with the ‘^’ indicating the correct order.
This is absolutely crucial for the analysis of Hebrew, which has many bound morphemes that carry their own POS-tag.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Trivia break!&lt;/strong&gt;&lt;br /&gt;
There is a &lt;strong&gt;single&lt;/strong&gt; concept/meaning that is conveyed by a &lt;strong&gt;bound morpheme in English&lt;/strong&gt; and a &lt;strong&gt;free morpheme in Hebrew&lt;/strong&gt; (it’s usually the other way around!), can you find it? Hint in the comments.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Another major challenge in Hebrew is that some morphemes are covert (due to orthographic rules), so the internal structure of a word doesn’t necessarily correspond to the surface form. For example the word ב-בית /ba-bayit/ (‘in the house’) has two morphemes on the surface - in + house, and the definite article is covert (if there wasn’t a definite article it would be בבית /b&lt;strong&gt;e&lt;/strong&gt;-bayit/).&lt;/p&gt;

&lt;p&gt;Lastly, Hebrew has an intertwined nature (a.k.a non-concatenative morphology), which for our purposes means that words can’t necessarily be segmented linearly.&lt;/p&gt;

&lt;h3 id=&quot;working-through-the-challenges-to-find-more-challenges&quot;&gt;Working through the challenges to find… more challenges…&lt;/h3&gt;
&lt;p&gt;The widely-accepted conclusion is that in order to parse Hebrew correctly we must first segment each word to its composing morphemes as part of necessary pre-processing, and then we can continue with the regular pipeline like we do for English. 
However, it’s usually the case that a single word would have more than one possible segmentation, non of which is a-priori more likely, and the correct one is only recoverable in &lt;strong&gt;context&lt;/strong&gt;…. see where this is going, right?&lt;/p&gt;

&lt;h3 id=&quot;along-came-bert&quot;&gt;Along came BERT&lt;/h3&gt;
&lt;p&gt;Not going to introduce BERT here, it took the world by storm and since then has been used for pretty much anything - turning regular rocks into gold, curing the blind, bringing people back from the dead… and wasn’t trained on any of these tasks, what a guy!&lt;br /&gt;
&lt;em&gt;(Hi, this is me being totally sarcastic because it’s my blog. As you’ll see here and in the future, I’m not the biggest fan of huge-but-dumb models that happen to work well for English)&lt;/em&gt;&lt;br /&gt;
So BERT has four important qualities that make it interesting for multi-tagging Hebrew:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;It’s a contextualized model (remember that the correct segmentation relies on context).&lt;/li&gt;
  &lt;li&gt;It (linearly…) segments words into sub-word units called word-pieces.&lt;/li&gt;
  &lt;li&gt;It’s multilingual and the Hebrew part of it is trained on a larger corpus than previous pre-neural Hebrew models used.&lt;/li&gt;
  &lt;li&gt;It’s really important for reviewers.&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;and-heres-where-the-study-actually-begins&quot;&gt;And here’s where the study actually begins..&lt;/h3&gt;
&lt;p&gt;After a long introduction we can now say that this study focuses on the 2nd point from BERT’s qualities, and that is its segmentation process. We have established that we need segmentation of words, and implicitly meant that we were looking for the &lt;strong&gt;correct&lt;/strong&gt; morphological segmentation. But what if we can’t get the correct segmentation? can we safely use BERT’s segmentation and succeed on a relatively simple task like POS-tagging?&lt;/p&gt;

&lt;h3 id=&quot;hypothesis-and-approach&quot;&gt;Hypothesis and Approach&lt;/h3&gt;
&lt;p&gt;Since the word-pieces themselves don’t reflect the actual morphemes, we hypothesize that segmentation into word-pieces will deteriorate performance for multi-tagging Hebrew. By deteriorating performance we mean that either the accuracy levels will go down, or access to internal structure will be lost. We show how incorporating linguistic knowledge helps maintain access to internal structure as well as improving overall accuracy.&lt;/p&gt;

&lt;h3 id=&quot;some-experimental-settings&quot;&gt;Some Experimental Settings&lt;/h3&gt;
&lt;p&gt;The running example throughout will be of the word בבית /’in the house’/, for which the relevant multi-tag is IN^DEF\^NN and the BPE . Broadly speaking, because we only fine tune BERT and not changing the segmentation algorithm (something that would require pre-training from scratch) all we have left to play with are the tags that the word-pieces (WPs from now) receive. Here there are two strategies, one where all the WPs of a given word get the same tag, and another where each WP can get a different tag. The common practice is to give all the WPs the same tag, so let’s start with that strategy.&lt;br /&gt;
&lt;strong&gt;Predicting the entire multi-tag at word level&lt;/strong&gt;&lt;/p&gt;
&lt;figure class=&quot;image&quot;&gt;&lt;img src=&quot;https://github.com/stavkl/linguistics-for-nlp/raw/master/images/iscol-post/raw.JPG&quot; width=&quot;25%&quot; align=&quot;right&quot; alt=&quot;&quot; /&gt;

Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus tincidunt leo vel lorem varius, non bibendum lectus volutpat. Suspendisse id augue et ipsum iaculis luctus. Vivamus luctus, ligula euismod pharetra luctus, dolor ex dictum velit, ut tempus mauris purus vel ante. Proin euismod ante sed magna ultrices rhoncus. Nulla et sollicitudin orci. Pellentesque at libero ex. Duis nec lorem eu leo euismod tempus. Fusce semper ultrices felis, id lacinia turpis faucibus nec. In posuere pellentesque arcu accumsan pharetra.
&lt;/figure&gt;</content><author><name></name></author><summary type="html">This post reviews the paper I presented at SIGMORPHON this year (see link in the publications section), that should have been presented at the ISCOL conference too but eventually didn’t.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://stavkl.github.io/linguistics-for-nlp/images/bert.png" /><media:content medium="image" url="https://stavkl.github.io/linguistics-for-nlp/images/bert.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry></feed>