{
  
    
        "post0": {
            "title": "Getting the ###life out of Living: How adequate are word-pieces for modelling complex morphology?",
            "content": "This post reviews the paper I presented at SIGMORPHON this year (see link in the publications section), that should have been presented at the ISCOL conference too but eventually didn’t. . . How adequate are word-pieces for modelling complex morphology? . tldr; not at all. . The task: Part-of-speech Tagging The language: Hebrew The data: The Hebrew treebank The means: fine-tuning Multilingual BERT . The challenges with POS-tagging in Hebrew . This section only deals with the challenges in POS-tagging Hebrew, and in another post I’ll review the challenges of processing languages with non-concatenative morphology in general. In NLP, a word is a space-delimited sequence of characters. Each word is composed of at least one morpheme. A morpheme is the smallest unit of meaning, and it comes in two shapes: bound and free. A bound morpheme has to be part of a word, like the English plural suffix -s, while a free morpheme can stand on its own, like the coordinator and. . The main difference between English and Hebrew in this sense is that Hebrew has a much higher morphemes-to-word ratio than English, so for example the sequence and when I saw (4 free morphemes in English) is expressed in Hebrew using a single word - וכשראיתי /ve-kshe-raiti/ (and - bound, when - bound, saw.1st per.sg - free). Each morpheme in the Hebrew word has a different POS tag, which one should we choose? Easy, we don’t, we take them all (otherwise we loose valuable syntactic information that we do encode for English) - Introducing the multi-tag, which is a POS-tag composed of POS-tags. In the example the multi-tag would be CONJ^REL ^VB with the ‘^’ indicating the correct order. This is absolutely crucial for the analysis of Hebrew, which has many bound morphemes that carry their own POS-tag. . Trivia break! There is a single concept/meaning that is conveyed by a bound morpheme in English and a free morpheme in Hebrew (it’s usually the other way around!), can you find it? Hint in the comments. . Another major challenge in Hebrew is that some morphemes are covert (due to orthographic rules), so the internal structure of a word doesn’t necessarily correspond to the surface form. For example the word ב-בית /ba-bayit/ (‘in the house’) has two morphemes on the surface - in + house, and the definite article is covert (if there wasn’t a definite article it would be בבית /be-bayit/). . Lastly, Hebrew has an intertwined nature (a.k.a non-concatenative morphology), which for our purposes means that words can’t necessarily be segmented linearly. . Working through the challenges to find… more challenges… . The widely-accepted conclusion is that in order to parse Hebrew correctly we must first segment each word to its composing morphemes as part of necessary pre-processing, and then we can continue with the regular pipeline like we do for English. However, it’s usually the case that a single word would have more than one possible segmentation, non of which is a-priori more likely, and the correct one is only recoverable in context…. see where this is going, right? . Along came BERT . Not going to introduce BERT here, it took the world by storm and since then has been used for pretty much anything - turning regular rocks into gold, curing the blind, bringing people back from the dead… and wasn’t trained on any of these tasks, what a guy! (Hi, this is me being totally sarcastic because it’s my blog. As you’ll see here and in the future, I’m not the biggest fan of huge-but-dumb models that happen to work well for English) So BERT has four important qualities that make it interesting for multi-tagging Hebrew: . It’s a contextualized model (remember that the correct segmentation relies on context). | It (linearly…) segments words into sub-word units called word-pieces. | It’s multilingual and the Hebrew part of it is trained on a larger corpus than previous pre-neural Hebrew models used. | It’s really important for reviewers. | And here’s where the study actually begins.. . After a long introduction we can now say that this study focuses on the 2nd point from BERT’s qualities, and that is its segmentation process. We have established that we need segmentation of words, and implicitly meant that we were looking for the correct morphological segmentation. But what if we can’t get the correct segmentation? can we safely use BERT’s segmentation and succeed on a relatively simple task like POS-tagging? . Hypothesis and Approach . Since the word-pieces themselves don’t reflect the actual morphemes, we hypothesize that segmentation into word-pieces will deteriorate performance for multi-tagging Hebrew. By deteriorating performance we mean that either the accuracy levels will go down, or access to internal structure will be lost. We show how incorporating linguistic knowledge helps maintain access to internal structure as well as improving overall accuracy. . Some Experimental Settings . The running example throughout will be of the word בבית /’in the house’/, for which the relevant multi-tag is IN^DEF^NN and the BPE . Broadly speaking, because we only fine tune BERT and not changing the segmentation algorithm (something that would require pre-training from scratch) all we have left to play with are the tags that the word-pieces (WPs from now) receive. Here there are two strategies, one where all the WPs of a given word get the same tag, and another where each WP can get a different tag. The common practice is to give all the WPs the same tag, so let’s start with that strategy. . Predicting the entire multi-tag at word level . In this setting we predict for each WP the complete multi-tag of the entire word, that is, for the WP ‘ב’ predict the multi tag IN^DEF^NN and also for the second WP ‘##בית’. We choose the prediction of the first WP to be the multi-tag for the whole word. On the upside, this is the easiest method to apply, but nothing comes without a price - we cannot generalize to new unseen tags, so if the model haven’t seen the tag IN^DEF^NN during fine-tuning it won’t be able to predict it in the evaluation. Also, the inner structure of the word remain inaccessible - eventually the model’s prediction is that the word בבית has a multi-tag of IN^DEF^NN, but we can’t infer which part of the word contributed which tag, we can only tell that it’s there. For POS tagging that might not make much of a difference, but for other tasks down the pipeline, like NER, it’s really important to recognize where a proper noun begins and ends, for example. The F1 score for this setting is 94.09. We can do better! We just need to be a little more sophisticated with the linguistic knowledge that we have and still didn’t use. . Predicting Prefix (multi)-tag and the host (multi)-tag . Hebrew words, as described before, are composed of morphemes, some morphemes come before the stem (called prefixes, like dis- in English), and some come after (like -ness). While in English affixes don’t usually carry a separate POS-tag, in Hebrew they do. In this section we use the fact that we can separate, fairly easily, the prefix from the rest of the word (which we’ll call the host). Separating the suffix is also possible but not as easily, so we didn’t do that. . We fine-tune the model twice, once for the prefix classifier and once for the host classifier. Here the prefix classifier only predicts prefixes - it predicts IN^DEF for both WPs, and the host classifier predicts NN for both WPs. We then combine the results to a single tag IN^DEF^NN. . Because we split and reassemble the tags we can generalize better to unseen tags, and we also get more access to the inner structure as the distinction between prefix and host is essentially a distinction between function and content (respectively), and it makes sense to fine-tune it separately. It yields a slightly better F1 score of 94.22. . Strategy Summary . So far we see that incorporating linguistic knowledge can help in both accuracy and access to internal structure. Next we’ll see what we can get when we step out of the common practices. .",
            "url": "https://stavkl.github.io/linguistics-for-nlp/fastpages/jupyter/2020/09/21/getting-the-life.html",
            "relUrl": "/fastpages/jupyter/2020/09/21/getting-the-life.html",
            "date": " • Sep 21, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About",
          "content": "Hi :) . My name is Stav Klein. I’m studying towards an MA in Linguistics (Tel-Aviv University) while doing some cool stuff with NLP. I’m mostly interested in processing morphologically-rich low-resource languages, first-language acquisition and how to use linguistic knowledge to improve models, methodologies and applications (“just use more data” is never the answer). . Always happy to chat, you can find me on Twitter, Linkedin or Github . Moving on up",
          "url": "https://stavkl.github.io/linguistics-for-nlp/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
      ,"page3": {
          "title": "Publications",
          "content": "2020 . Notes on Modern Hebrew phonology and orthography (book chapter)Stav Klein. Usage-Based Studies in Modern Hebrew - Background, Morpho-lexicon and Syntax. . Getting the ##life out of living: How adequate are word-pieces for modelling complex morphology?Stav Klein and Reut Tsarfaty. SIGMORPHON, ACL workshop. . From SPMRL to NMRL: What Did We Learn (and Unlearn) in a Decade of Parsing Morphologically-Rich Languages (MRLs)?Reut Tsarfaty, Dan Bareket, Stav Klein, Amit Seker. ACL. . 2019 . What’s Wrong with Hebrew NLP? And How to Make it RightReut Tsarfaty, Shoval Sadde, Stav Klein, Amit Seker. EMNLP Demo paper. .",
          "url": "https://stavkl.github.io/linguistics-for-nlp/publications/",
          "relUrl": "/publications/",
          "date": ""
      }
      
  

  
  

  

  
  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://stavkl.github.io/linguistics-for-nlp/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}