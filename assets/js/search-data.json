{
  
    
        "post0": {
            "title": "Getting the life out of Living",
            "content": "Importing the libraries . Note that the &#39;bclm&#39; library is an internal library developed in the ONLP lab, and is currently unavailable for public use. All the results are obtailable and reproducible with the standard parsing of CONLL formatted files (all available in the ONLP Github page). . import os import csv import pandas as pd import numpy as np from tqdm import tqdm, trange import bclm import torch from torch.optim import Adam from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler from keras.preprocessing.sequence import pad_sequences from sklearn.model_selection import train_test_split from sklearn.metrics import classification_report from transformers import BertTokenizer, BertConfig from transformers import BertForTokenClassification, AdamW . Using TensorFlow backend. . Manually setting seeds . Results are calculated based on an average of 5 independent runs of this code. The seeds are set to minimize the variation between runs. Due to internal randomization in pytorch results can&#39;t be identical from run to run even when using the same seeds (see pytorch documentation) . torch.manual_seed(3) np.random.seed(3) torch.cuda.manual_seed_all(3) torch.backends.cudnn.deterministic = True torch.backends.cudnn.benchmark = False . Taking a first and important look at the data . Data is split to train, dev and test sets. We fine-tune the model on the train set and evaluate it on the dev set. . train = bclm.read_dataframe(&#39;spmrl&#39;, subset=&#39;train&#39;) train_df = bclm.get_token_df(train, [&#39;upostag&#39;]) train_df[&#39;token_str&#39;] = train_df[&#39;token_str&#39;].str.replace(&#39;”&#39;,&#39;&quot;&#39;) dev = bclm.read_dataframe(&#39;spmrl&#39;, subset=&#39;dev&#39;) dev_df = bclm.get_token_df(dev, [&#39;upostag&#39;]) dev_df[&#39;token_str&#39;] = dev_df[&#39;token_str&#39;].str.replace(&#39;”&#39;,&#39;&quot;&#39;) . Here are the first lines of the dev set . Notice that some words have a multi-tag while other words carry a simple tag - a challenge for Hebrew POS-tagging. . dev_df.head(20) . sent_id token_id token_str upostag set . 0 1 | 1 | עשרות | CDT | dev | . 1 1 | 2 | אנשים | NN | dev | . 2 1 | 3 | מגיעים | BN | dev | . 3 1 | 4 | מתאילנד | PREPOSITION^NNP | dev | . 4 1 | 5 | לישראל | PREPOSITION^NNP | dev | . 5 1 | 6 | כשהם | TEMP^PRP | dev | . 6 1 | 7 | נרשמים | BN | dev | . 7 1 | 8 | כמתנדבים | PREPOSITION^NN | dev | . 8 1 | 9 | , | yyCM | dev | . 9 1 | 10 | אך | CC | dev | . 10 1 | 11 | למעשה | RB | dev | . 11 1 | 12 | משמשים | BN | dev | . 12 1 | 13 | עובדים | NN | dev | . 13 1 | 14 | שכירים | JJ | dev | . 14 1 | 15 | זולים | JJ | dev | . 15 1 | 16 | . | yyDOT | dev | . 16 2 | 1 | תופעה | NN | dev | . 17 2 | 2 | זו | PRP | dev | . 18 2 | 3 | התבררה | VB | dev | . 19 2 | 4 | אתמול | RB | dev | . Uniform column names . We evaluated on different datasets so a uniform column name was needed . train_df.rename(columns = {&quot;token_str&quot;: &quot;form&quot;}, inplace = True) dev_df.rename(columns = {&quot;token_str&quot;: &quot;form&quot;}, inplace = True) . Get lists of sentences and their corresponding labels . Includes an example of a sentence from the train set at the end. Also, not shown here, we removed the four longest sentences from the dev set, as they caused systematic issued later on in the code. . class sentenceGetter(object): def __init__(self, data, max_sent=None): self.index = 0 self.max_sent = max_sent self.tokens = data[&#39;form&#39;] self.labels = data[&#39;upostag&#39;] #for evaluating by word-accuracy self.correspondingToken = data[&#39;token_id&#39;] self.orig_sent_id = data[&#39;sent_id&#39;] def sentences(self): sent = [] counter = 0 for token,label, corres_tok, sent_id in zip(self.tokens, self.labels, self.correspondingToken, self.orig_sent_id): sent.append((token, label, corres_tok, sent_id)) if token.strip() == &quot;.&quot;: yield sent sent = [] counter += 1 if self.max_sent is not None and counter &gt;= self.max_sent: return train_getter = sentenceGetter(train_df) dev_getter = sentenceGetter(dev_df) test_getter = sentenceGetter(test_df) train_sentences = [[token for token, label, corres_tok, sent_id in sent] for sent in train_getter.sentences()] train_labels = [[label for token, label, corres_tok, sent_id in sent] for sent in train_getter.sentences()] dev_sentences = [[token for token, label, corres_tok, sent_id in sent] for sent in dev_getter.sentences()] dev_labels = [[label for token, label, corres_tok, sent_id in sent] for sent in dev_getter.sentences()] dev_corresTokens = [[corres_tok for token, label, corres_tok, sent_id in sent] for sent in dev_getter.sentences()] dev_sent_ids = [[sent_id for token, label, corres_tok, sent_id in sent] for sent in dev_getter.sentences()] print(train_sentences[10]) print(train_labels[10]) print(len(dev_sentences)) print(len(test_sentences)) . [&#39;הם&#39;, &#39;התבקשו&#39;, &#39;לדווח&#39;, &#39;למשטרה&#39;, &#39;על&#39;, &#39;תנועותיהם&#39;, &#39;.&#39;] [&#39;PRP&#39;, &#39;VB&#39;, &#39;VB&#39;, &#39;PREPOSITION^DEF^NN&#39;, &#39;IN&#39;, &#39;NN&#39;, &#39;yyDOT&#39;] 490 712 . Put everything on CUDA . device = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;) n_gpu = torch.cuda.device_count() torch.cuda.set_device(0) print(&quot;Device: &quot; + str(device)) print(&quot;Number of gpus: &quot; + str(n_gpu)) print(&quot;Name of gpu: &quot; + torch.cuda.get_device_name(0)) . Device: cuda Number of gpus: 4 Name of gpu: GeForce RTX 2080 Ti . MAX_LEN = 150 bs = 32 . Tokenize the training set (with BERT) . See the example of how the tokenization looks like in the end . tokenizer = BertTokenizer.from_pretrained(&#39;bert-base-multilingual-cased&#39;, do_lower_case=False) def tokenize(sentences, orig_labels): tokenized_texts = [] labels = [] for sent, sent_labels in zip(sentences, orig_labels): bert_tokens = [] bert_labels = [] for orig_token, orig_label in zip(sent, sent_labels): b_tokens = tokenizer.tokenize(orig_token) bert_tokens.extend(b_tokens) for b_token in b_tokens: bert_labels.append(orig_label) tokenized_texts.append(bert_tokens) labels.append(bert_labels) assert len(bert_tokens) == len(bert_labels) return tokenized_texts, labels train_tokenized_texts, train_tokenized_labels = tokenize(train_sentences, train_labels) print(train_tokenized_texts[10]) print(train_tokenized_labels[10]) . [&#39;הם&#39;, &#39;ה&#39;, &#39;##ת&#39;, &#39;##בק&#39;, &#39;##שו&#39;, &#39;ל&#39;, &#39;##דו&#39;, &#39;##וח&#39;, &#39;ל&#39;, &#39;##משטרה&#39;, &#39;על&#39;, &#39;ת&#39;, &#39;##נוע&#39;, &#39;##ות&#39;, &#39;##יהם&#39;, &#39;.&#39;] [&#39;PRP&#39;, &#39;VB&#39;, &#39;VB&#39;, &#39;VB&#39;, &#39;VB&#39;, &#39;VB&#39;, &#39;VB&#39;, &#39;VB&#39;, &#39;PREPOSITION^DEF^NN&#39;, &#39;PREPOSITION^DEF^NN&#39;, &#39;IN&#39;, &#39;NN&#39;, &#39;NN&#39;, &#39;NN&#39;, &#39;NN&#39;, &#39;yyDOT&#39;] . Get a list of all possible POS-tag labels . Note that this list contains simple and multi POS tags. There are 50 Simple POS-tags (in isolation), and 315 POS-tags (combining simple and multi POS-tags). Making even more complex tags (for example, joining the &#39;feats&#39; tag as well) results in an exponentially larger and more sparse label space. . data = train_df tag_vals = list(set(data[&quot;upostag&quot;].values)) tags = [&#39;PAD&#39;] + tag_vals tag2idx = {tag:idx for idx, tag in enumerate(tags)} idx2tag = {idx:tag for idx, tag in enumerate(tags)} print(tag2idx) # print(idx2tag) print(len(tags)) . {&#39;PAD&#39;: 0, &#39;REL^PREPOSITION^CDT&#39;: 1, &#39;DEF^DTT&#39;: 2, &#39;CONJ^VB^AT^PRP&#39;: 3, &#39;CONJ^yyQUOT^NNT&#39;: 4, &#39;PREPOSITION^ADVERB^CD&#39;: 5, &#39;TEMP^PRP&#39;: 6, &#39;CONJ^TEMP^RB&#39;: 7, &#39;AT&#39;: 8, &#39;CONJ^REL^COP&#39;: 9, &#39;PREPOSITION^ADVERB^NCD&#39;: 10, &#39;PREPOSITION^ADVERB^CDT&#39;: 11, &#39;CONJ^DTT&#39;: 12, &#39;TEMP^PREPOSITION^DEF^NN&#39;: 13, &#39;TEMP^NNP&#39;: 14, &#39;IN^IN^NNT&#39;: 15, &#39;DEF^P&#39;: 16, &#39;ADVERB^DTT&#39;: 17, &#39;VB^AT^S_ANP&#39;: 18, &#39;CONJ^DEF^NNP&#39;: 19, &#39;CD&#39;: 20, &#39;PREPOSITION^PREPOSITION^DEF^PRP&#39;: 21, &#39;PREPOSITION^yyQUOT^DEF^NN&#39;: 22, &#39;REL^DTT&#39;: 23, &#39;CONJ^IN&#39;: 24, &#39;POS&#39;: 25, &#39;PREPOSITION^JJ&#39;: 26, &#39;TEMP^NNT&#39;: 27, &#39;REL^yyQUOT^NNP&#39;: 28, &#39;CONJ^NNP&#39;: 29, &#39;IN^NN&#39;: 30, &#39;PREPOSITION^CDT&#39;: 31, &#39;PREPOSITION^NNP&#39;: 32, &#39;CONJ^CC&#39;: 33, &#39;yyLRB&#39;: 34, &#39;DEF^NN&#39;: 35, &#39;BNT&#39;: 36, &#39;REL^DEF^BN&#39;: 37, &#39;REL^yyQUOT^JJ&#39;: 38, &#39;PREPOSITION^DEF^yyQUOT^NNP&#39;: 39, &#39;REL&#39;: 40, &#39;NN&#39;: 41, &#39;IN^NNT&#39;: 42, &#39;TEMP^RB&#39;: 43, &#39;DEF^RB&#39;: 44, &#39;REL^JJ&#39;: 45, &#39;IN&#39;: 46, &#39;JJ&#39;: 47, &#39;PREPOSITION^DEF^CD&#39;: 48, &#39;CONJ^REL^PREPOSITION^NN&#39;: 49, &#39;CONJ^yyQUOT^NN&#39;: 50, &#39;CONJ^PREPOSITION^CDT&#39;: 51, &#39;CONJ^MD&#39;: 52, &#39;IN^PRP&#39;: 53, &#39;CONJ^yyQUOT^DEF^JJ&#39;: 54, &#39;PREPOSITION^POS^S_PRN&#39;: 55, &#39;PREPOSITION^CC&#39;: 56, &#39;CONJ^INTJ&#39;: 57, &#39;CONJ^PRP&#39;: 58, &#39;CONJ^REL^BN&#39;: 59, &#39;PREPOSITION^JJT&#39;: 60, &#39;REL^PREPOSITION^DTT&#39;: 61, &#39;PREPOSITION^DEF^BN&#39;: 62, &#39;IN^VB&#39;: 63, &#39;PREPOSITION^DEF&#39;: 64, &#39;REL^DT&#39;: 65, &#39;CONJ^yyQUOT^IN&#39;: 66, &#39;TEMP^VB&#39;: 67, &#39;CONJ^DEF^JJ&#39;: 68, &#39;CONJ^IN^DEF^NN&#39;: 69, &#39;PREPOSITION^ADVERB^NN&#39;: 70, &#39;REL^COP&#39;: 71, &#39;REL^BN&#39;: 72, &#39;PREPOSITION^yyQUOT^NN&#39;: 73, &#39;CONJ^IN^PRP&#39;: 74, &#39;CONJ^REL^NNT&#39;: 75, &#39;CONJ^PREPOSITION^NN&#39;: 76, &#39;yyCLN&#39;: 77, &#39;yyQM&#39;: 78, &#39;ZVL^PREPOSITION^NNT&#39;: 79, &#39;REL^MD&#39;: 80, &#39;CONJ^PREPOSITION^CC&#39;: 81, &#39;REL^AT&#39;: 82, &#39;ZVL^DEF^NNT&#39;: 83, &#39;PREPOSITIONIN^PREPOSITION^NN&#39;: 84, &#39;PREPOSITION^DEF^NN&#39;: 85, &#39;REL^DEF^JJ&#39;: 86, &#39;DT&#39;: 87, &#39;VB&#39;: 88, &#39;CONJ&#39;: 89, &#39;IN^IN&#39;: 90, &#39;PREPOSITION^IN&#39;: 91, &#39;CONJ^CDT&#39;: 92, &#39;PREPOSITION^REL^VB&#39;: 93, &#39;CONJ^REL^VB&#39;: 94, &#39;PREPOSITION^VB&#39;: 95, &#39;REL^CC&#39;: 96, &#39;ZVL^PREPOSITION^DEF^NN&#39;: 97, &#39;PREPOSITION^PREPOSITION^DEF^NN&#39;: 98, &#39;CONJ^COP&#39;: 99, &#39;REL^yyQUOT^COP&#39;: 100, &#39;IN^NCD&#39;: 101, &#39;CONJ^yyQUOT^NNP&#39;: 102, &#39;CONJ^PREPOSITION^DEF^NNP&#39;: 103, &#39;CONJ^DEF^CD&#39;: 104, &#39;PREPOSITION^PREPOSITION^DEF^DEF&#39;: 105, &#39;PREPOSITION^REL^COP&#39;: 106, &#39;AT^S_PRN&#39;: 107, &#39;CONJ^IN^S_PRN&#39;: 108, &#39;DEF^yyQUOT^JJ&#39;: 109, &#39;DEF^MD&#39;: 110, &#39;CONJ^QW&#39;: 111, &#39;yyDOT&#39;: 112, &#39;CONJ^RB&#39;: 113, &#39;CC&#39;: 114, &#39;REL^PREPOSITION^DEF^PRP&#39;: 115, &#39;REL^IN^PRP&#39;: 116, &#39;PREPOSITION^DEF^CDT&#39;: 117, &#39;IN^CDT&#39;: 118, &#39;PREPOSITION^DEF^RB&#39;: 119, &#39;REL^PREPOSITION^RB&#39;: 120, &#39;IN^RB&#39;: 121, &#39;CONJ^DT&#39;: 122, &#39;CONJ^TEMP^VB&#39;: 123, &#39;REL^yyQUOT^MD&#39;: 124, &#39;CONJ^PREPOSITION^PRP&#39;: 125, &#39;PREPOSITION^DEF^JJ&#39;: 126, &#39;NN^yyDOT&#39;: 127, &#39;REL^yyQUOT^PRP&#39;: 128, &#39;CONJ^PREPOSITION^NNP&#39;: 129, &#39;MD&#39;: 130, &#39;CONJ^PREPOSITION^DEF^CD&#39;: 131, &#39;CONJ^JJ&#39;: 132, &#39;CONJ^PREPOSITION^BN&#39;: 133, &#39;yySCLN&#39;: 134, &#39;DEF^JJ&#39;: 135, &#39;PREPOSITION^PREPOSITION^NN&#39;: 136, &#39;PREPOSITION^IN^DEF^PRP&#39;: 137, &#39;CONJ^DEF^NN&#39;: 138, &#39;CONJ^DEF^yyQUOT^NN&#39;: 139, &#39;REL^DEF^NN&#39;: 140, &#39;ZVL^PREPOSITION^NN&#39;: 141, &#39;REL^ADVERB^CD&#39;: 142, &#39;PREPOSITION^CD&#39;: 143, &#39;REL^yyQUOT^BN&#39;: 144, &#39;INTJ&#39;: 145, &#39;REL^PRP&#39;: 146, &#39;VB^AT^PRP&#39;: 147, &#39;REL^PREPOSITION^yyQUOT^NNP&#39;: 148, &#39;CC^ZVL^DEF^NN&#39;: 149, &#39;RB&#39;: 150, &#39;CONJ^PREPOSITION^RB&#39;: 151, &#39;yyDASH&#39;: 152, &#39;CONJ^REL^IN&#39;: 153, &#39;TEMP^DEF^BN&#39;: 154, &#39;DEF^CD&#39;: 155, &#39;ZVL^CD&#39;: 156, &#39;CONJ^PREPOSITION^CD&#39;: 157, &#39;DEF^COP&#39;: 158, &#39;PRP&#39;: 159, &#39;CONJ^AT&#39;: 160, &#39;VB^AT^S_PRN&#39;: 161, &#39;NNP&#39;: 162, &#39;CONJ^PREPOSITION^BNT&#39;: 163, &#39;P&#39;: 164, &#39;CONJ^BN&#39;: 165, &#39;PREPOSITION^yyQUOT^NNT&#39;: 166, &#39;yyEXCL&#39;: 167, &#39;JJT&#39;: 168, &#39;REL^VB&#39;: 169, &#39;CONJ^PREPOSITION^yyQUOT^CDT&#39;: 170, &#39;TEMP^DEF^NN&#39;: 171, &#39;REL^DEF^NNP&#39;: 172, &#39;CONJ^VB&#39;: 173, &#39;PREPOSITION^PRP&#39;: 174, &#39;DEF^BN^AT^PRP&#39;: 175, &#39;yyELPS&#39;: 176, &#39;P^NN&#39;: 177, &#39;REL^PREPOSITION^DT&#39;: 178, &#39;CONJ^REL^DEF^NN&#39;: 179, &#39;IN^S_PRN&#39;: 180, &#39;NCD&#39;: 181, &#39;PREPOSITION^DT&#39;: 182, &#39;DEF^PREPOSITION^NNT&#39;: 183, &#39;REL^CDT&#39;: 184, &#39;DEF&#39;: 185, &#39;REL^JJT&#39;: 186, &#39;TEMP^IN&#39;: 187, &#39;yyQUOT&#39;: 188, &#39;CONJ^IN^NNT&#39;: 189, &#39;QW&#39;: 190, &#39;POS^S_PRN&#39;: 191, &#39;CONJ^NN&#39;: 192, &#39;CONJ^REL^EX&#39;: 193, &#39;CONJ^PREPOSITION^P&#39;: 194, &#39;PREPOSITION^BN&#39;: 195, &#39;ZVL^DEF^NNP&#39;: 196, &#39;PREPOSITION^NNT&#39;: 197, &#39;ZVL^JJT&#39;: 198, &#39;IN^NNP&#39;: 199, &#39;REL^PREPOSITION^BN&#39;: 200, &#39;CONJ^yyQUOT^DEF^NN&#39;: 201, &#39;PREPOSITION^DEF^P&#39;: 202, &#39;TEMP^COP&#39;: 203, &#39;REL^AT^S_PRN&#39;: 204, &#39;PREPOSITION^yyQUOT^NNP&#39;: 205, &#39;REL^P&#39;: 206, &#39;PREPOSITION^yyQUOT^PREPOSITION^NNT&#39;: 207, &#39;ZVL^ZVL&#39;: 208, &#39;REL^PREPOSITION^NNP&#39;: 209, &#39;TEMP^PREPOSITION^NNP&#39;: 210, &#39;NEG&#39;: 211, &#39;CONJ^PREPOSITION^DTT&#39;: 212, &#39;REL^yyQUOT^PREPOSITION^NN&#39;: 213, &#39;REL^DEF^CD&#39;: 214, &#39;CONJ^NNT&#39;: 215, &#39;PREPOSITION^RB^S_PRN&#39;: 216, &#39;DEF^NNP&#39;: 217, &#39;CONJ^POS&#39;: 218, &#39;REL^yyQUOT^DEF^NN&#39;: 219, &#39;PREPOSITION^ PREPOSITION^DEF^NN&#39;: 220, &#39;DEF^NCD&#39;: 221, &#39;REL^NNT&#39;: 222, &#39;PREPOSITION^DEF^NNT&#39;: 223, &#39;IN^REL^NNT&#39;: 224, &#39;ZVL^RB&#39;: 225, &#39;BN&#39;: 226, &#39;PREPOSITION^NN&#39;: 227, &#39;REL^PREPOSITION^NN&#39;: 228, &#39;TEMP^PREPOSITION^NN&#39;: 229, &#39;ZVL^NNT&#39;: 230, &#39;TEMP^NN&#39;: 231, &#39;PREPOSITION^BNT&#39;: 232, &#39;CDT&#39;: 233, &#39;DEF^yyQUOT^NNP&#39;: 234, &#39;PREPOSITION^yyQUOT^PREPOSITION^DEF^NN&#39;: 235, &#39;PREPOSITION^IN^S_PRN&#39;: 236, &#39;ZVL^DEF^NN&#39;: 237, &#39;CONJ^EX&#39;: 238, &#39;DEF^DEF^NN&#39;: 239, &#39;CONJ^IN^JJT&#39;: 240, &#39;NNT&#39;: 241, &#39;DTT&#39;: 242, &#39;IN^yyQUOT^VB&#39;: 243, &#39;REL^RB&#39;: 244, &#39;EX&#39;: 245, &#39;CONJ^PREPOSITION^QW&#39;: 246, &#39;ADVERB^NCD&#39;: 247, &#39;TEMP^BN&#39;: 248, &#39;CONJ^JJT&#39;: 249, &#39;CONJ^PREPOSITION^yyQUOT^NNP&#39;: 250, &#39;IN^DEF^NN&#39;: 251, &#39;CONJ^CD&#39;: 252, &#39;REL^CD&#39;: 253, &#39;DEF^NNT&#39;: 254, &#39;CONJ^BNT&#39;: 255, &#39;REL^PREPOSITION^NNT&#39;: 256, &#39;PREPOSITION^POS&#39;: 257, &#39;ZVL&#39;: 258, &#39;PREPOSITION^NEG&#39;: 259, &#39;PREPOSITION^yyQUOT^BN&#39;: 260, &#39;REL^QW&#39;: 261, &#39;CONJ^DEF^DTT&#39;: 262, &#39;REL^VB^AT^PRP&#39;: 263, &#39;CONJ^DEF^P&#39;: 264, &#39;ZVL^IN&#39;: 265, &#39;PREPOSITIONIN^NN&#39;: 266, &#39;REL^PREPOSITION^DEF^NN&#39;: 267, &#39;PREPOSITION^DEF^PRP&#39;: 268, &#39;PREPOSITION^NCD&#39;: 269, &#39;IN^JJT&#39;: 270, &#39;yyCM&#39;: 271, &#39;REL^IN&#39;: 272, &#39;REL^IN^S_PRN&#39;: 273, &#39;CONJ^DEF^MD&#39;: 274, &#39;REL^yyQUOT^RB&#39;: 275, &#39;REL^yyQUOT^VB&#39;: 276, &#39;PREPOSITION^PREPOSITION^NNT&#39;: 277, &#39;CONJ^yyQUOT^VB&#39;: 278, &#39;CONJ^REL^NN&#39;: 279, &#39;DEF^yyQUOT^NN&#39;: 280, &#39;REL^NN&#39;: 281, &#39;PREPOSITION&#39;: 282, &#39;COP&#39;: 283, &#39;REL^EX&#39;: 284, &#39;AT^PRP&#39;: 285, &#39;TEMP^PREPOSITION^PRP&#39;: 286, &#39;CONJ^PREPOSITION^DEF^NN&#39;: 287, &#39;IN^RB^CD&#39;: 288, &#39;REL^NNP&#39;: 289, &#39;CONJ^BN^AT^PRP&#39;: 290, &#39;ADVERB^NN&#39;: 291, &#39;CONJ^TEMP^BN&#39;: 292, &#39;CONJ^PREPOSITION^NNT&#39;: 293, &#39;CONJ^DEF^BN&#39;: 294, &#39;PREPOSITION^RB&#39;: 295, &#39;DEF^BN&#39;: 296, &#39;REL^yyQUOT^NN&#39;: 297, &#39;PREPOSITION^DEF^NNP&#39;: 298, &#39;CONJ^P&#39;: 299, &#39;ZVL^NNP&#39;: 300, &#39;IN^DTT&#39;: 301, &#39;ADVERB^CD&#39;: 302, &#39;CONJ^PREPOSITION^JJ&#39;: 303, &#39;ZVL^COP&#39;: 304, &#39;CONJ^PREPOSITION^DEF^JJ&#39;: 305, &#39;CONJ^PREPOSITION^DEF^PRP&#39;: 306, &#39;PREPOSITION^P&#39;: 307, &#39;RB^S_PRN&#39;: 308, &#39;PREPOSITION^QW&#39;: 309, &#39;ZVL^NN&#39;: 310, &#39;DEF^PRP&#39;: 311, &#39;PREPOSITION^DTT&#39;: 312, &#39;yyRRB&#39;: 313, &#39;TEMP^DEF^CD&#39;: 314} 315 . All the technical stuff . To make sentences and labels the same length and in tensor form . def pad_sentences_and_labels(tokenized_texts, labels): input_ids = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts], maxlen = MAX_LEN, dtype = &quot;float32&quot;, truncating = &quot;post&quot;, padding = &quot;post&quot;, value = tag2idx[&#39;PAD&#39;]) tags = pad_sequences([[tag2idx.get(l) for l in lab] for lab in labels], maxlen = MAX_LEN, value = tag2idx[&#39;PAD&#39;], padding = &quot;post&quot;, dtype = &quot;float32&quot;, truncating = &quot;post&quot;) attention_masks = [[float(i&gt;0) for i in ii] for ii in input_ids] return input_ids, tags, attention_masks input_ids, tags, attention_masks = pad_sentences_and_labels(train_tokenized_texts, train_tokenized_labels) . tr_inputs = torch.tensor(input_ids, dtype=torch.long) tr_tags = torch.tensor(tags, dtype=torch.long) tr_masks = torch.tensor(attention_masks, dtype=torch.long) train_data = TensorDataset(tr_inputs, tr_masks, tr_tags) train_sampler = RandomSampler(train_data) train_dataloader = DataLoader(train_data, sampler = train_sampler, batch_size=bs) . Performing the Fine-tuning . As you can see we didn&#39;t perform any hyperparameter search. This is intended. Of course, HP-optimization can make the model better (you are welcome to try it yourself), but we didn&#39;t want to make the best model, we wanted to investigate how well contextualized models can represent complex morphology, so all of our models use the same fine-tuning procedure described below, and the only thing that changes is the label that each word-piece receives (described in detail in the theoratical post on this paper). . from transformers import get_linear_schedule_with_warmup model = BertForTokenClassification.from_pretrained(&#39;bert-base-multilingual-cased&#39;, num_labels=len(tag2idx), output_attentions = False, output_hidden_states = False) model.cuda() FULL_FINETUNING = True if FULL_FINETUNING: param_optimizer = list(model.named_parameters()) no_decay = [&#39;bias&#39;, &#39;gamma&#39;, &#39;beta&#39;] optimizer_grouped_parameters = [ {&#39;params&#39;: [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], &#39;weight_decay_rate&#39;: 0.01}, {&#39;params&#39;: [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], &#39;weight_decay_rate&#39;: 0.0} ] else: param_optimizer = list(model.classifier.named_parameters()) optimizer_grouped_parameters = [{&quot;params&quot;: [p for n, p in param_optimizer]}] optimizer = AdamW(optimizer_grouped_parameters, lr=3e-5, eps=1e-8) from seqeval.metrics import f1_score def flat_accuracy(preds, labels): pred_flat = np.argmax(preds, axis=2).flatten() labels_flat = labels.flatten() # print (pred_flat, labels_flat) return np.sum(pred_flat == labels_flat) / len(labels_flat) epochs = 15 max_grad_norm = 1.0 # Total number of training steps is number of batches * number of epochs. total_steps = len(train_dataloader) * epochs # Create the learning rate scheduler. scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps) ## Store the average loss after each epoch so we can plot them. loss_values, validation_loss_values = [], [] for _ in trange(epochs, desc=&quot;Epoch&quot;): # TRAIN loop model.train() total_loss = 0 for step, batch in enumerate(train_dataloader): # add batch to gpu batch = tuple(t.to(device) for t in batch) b_input_ids, b_input_mask, b_labels = batch model.zero_grad() # forward pass outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels) # get the loss loss = outputs[0] # Perform a backward pass to calculate the gradients. loss.backward() # track train loss total_loss += loss.item() # Clip the norm of the gradient # This is to help prevent the &quot;exploding gradients&quot; problem. torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=max_grad_norm) # update parameters optimizer.step() # Update the learning rate. scheduler.step() # Calculate the average loss over the training data. avg_train_loss = total_loss / len(train_dataloader) print(&quot;Average train loss: {}&quot;.format(avg_train_loss)) # Store the loss value for plotting the learning curve. loss_values.append(avg_train_loss) . Epoch: 7%|▋ | 1/15 [00:58&lt;13:45, 58.94s/it] . Average train loss: 2.2720749048810256 . Epoch: 13%|█▎ | 2/15 [02:02&lt;13:05, 60.43s/it] . Average train loss: 0.821627008679666 . Epoch: 20%|██ | 3/15 [03:12&lt;12:36, 63.06s/it] . Average train loss: 0.5413947211284387 . Epoch: 27%|██▋ | 4/15 [04:27&lt;12:16, 66.92s/it] . Average train loss: 0.39871315207136304 . Epoch: 33%|███▎ | 5/15 [05:46&lt;11:42, 70.26s/it] . Average train loss: 0.31264687878520864 . Epoch: 40%|████ | 6/15 [07:04&lt;10:54, 72.67s/it] . Average train loss: 0.25154486896568223 . Epoch: 47%|████▋ | 7/15 [08:23&lt;09:56, 74.55s/it] . Average train loss: 0.20845407497529922 . Epoch: 53%|█████▎ | 8/15 [09:42&lt;08:50, 75.82s/it] . Average train loss: 0.17870022454544118 . Epoch: 60%|██████ | 9/15 [11:02&lt;07:43, 77.18s/it] . Average train loss: 0.15089197540165564 . Epoch: 67%|██████▋ | 10/15 [12:25&lt;06:34, 78.94s/it] . Average train loss: 0.13223850584932065 . Epoch: 73%|███████▎ | 11/15 [13:46&lt;05:17, 79.48s/it] . Average train loss: 0.11638171909573047 . Epoch: 80%|████████ | 12/15 [15:05&lt;03:58, 79.42s/it] . Average train loss: 0.10619803432277158 . Epoch: 87%|████████▋ | 13/15 [16:26&lt;02:39, 79.99s/it] . Average train loss: 0.09557023781694864 . Epoch: 93%|█████████▎| 14/15 [17:48&lt;01:20, 80.63s/it] . Average train loss: 0.09014363510926303 . Epoch: 100%|██████████| 15/15 [19:06&lt;00:00, 76.45s/it] . Average train loss: 0.08390635945589135 . . More Hebrew Fun . Remember that the model predicts a POS tag (simple or multi, one of the 315 from above) for each word-piece. But we don&#39;t want to evaluate word-pieces, we want to evaluate on whole words, because words can have multi-tags and also because we want to make a fair comparison with our baseline model that uses the actual morphemes (see example in the paper) - therefore all of our models are evaluated on word-level (as opposed to morpheme level, for example). . This setting of the aggregation of the wordpieces (multi-)tags to a whole word assumes we combine the (multi-)tag of each wordpiece to a single multi-tag. It can easily be adjusted for the case where we take the first (multi-)tag to be the tag for the whole word. . # Function receives a sentence with its labels, and the tokenized sentence and labels def aggr_toks_labels_tags(orig_words, orig_labels, tok_wordps, tok_labels, predicted_tags): joint_tokens = [] joint_labels = [] joint_predicted = [] for word in orig_words: aggregated_tokenized = &quot;&quot; aggregated_label = &quot;&quot; aggregated_predicted = &quot;&quot; aggregated_test = &quot;&quot; while aggregated_tokenized != word: tmpTok = tok_wordps.pop(0) if tmpTok.startswith(&quot;##&quot;): tmpTok = tmpTok[2:] tmpLab = tok_labels.pop(0) aggregated_label += &#39;^&#39; aggregated_label += tmpLab tmpPred = predicted_tags.pop(0) aggregated_predicted += &#39;^&#39; aggregated_predicted += tmpPred aggregated_tokenized += tmpTok joint_tokens.append(aggregated_tokenized) joint_labels.append(aggregated_label) joint_predicted.append(aggregated_predicted) assert len(joint_tokens) == len(orig_words) assert len(joint_tokens) == len(joint_predicted) return joint_tokens, joint_labels, joint_predicted . Building the evaluation models and evaluating on the dev set . def flat_accuracy(preds, labels): pred_flat = np.argmax(preds, axis=2).flatten() labels_flat = labels.flatten() return np.sum(pred_flat == labels_flat) / len(labels_flat) def delete_pads_from_preds(predicted_tags, test_tags): clean_predicted = [] clean_test = [] for ix in range(0, len(test_tags)): if test_tags[ix] != &#39;PAD&#39;: clean_predicted.append(predicted_tags[ix]) clean_test.append(test_tags[ix]) return clean_predicted, clean_test def calculate_accuracy(df): numOfCorrectPredictions = 0 for index in df.index: orig_pos = df.at[index, &#39;orig_label&#39;] pred_pos = df.at[index, &#39;predicted_tag&#39;] if orig_pos == pred_pos: numOfCorrectPredictions += 1 return numOfCorrectPredictions/len(df) def test_model(sentence, labels, tok_sent, tok_labels, corres_tokens, sent_id): input_ids, tags, attention_masks = pad_sentences_and_labels([tok_sent], [tok_labels]) val_inputs = torch.tensor(input_ids, dtype=torch.long) val_tags = torch.tensor(tags, dtype=torch.long) val_masks = torch.tensor(attention_masks, dtype=torch.long) test_data = TensorDataset(val_inputs, val_masks, val_tags) test_sampler = SequentialSampler(test_data) test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=bs) model.eval() eval_loss, eval_accuracy = 0, 0 nb_eval_steps, nb_eval_examples = 0, 0 predictions, true_labels = [], [] counter = 0 for batch in test_dataloader: batch = tuple(t.to(device) for t in batch) b_input_ids, b_input_mask, b_labels = batch with torch.no_grad(): outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels) logits = outputs[1].detach().cpu().numpy() label_ids = b_labels.to(&#39;cpu&#39;).numpy() predictions.append([list(p) for p in np.argmax(logits, axis=2)]) true_labels.append(label_ids) tmp_eval_accuracy = flat_accuracy(logits, label_ids) eval_loss += outputs[0].mean().item() eval_accuracy += flat_accuracy(logits, label_ids) nb_eval_examples += b_input_ids.size(0) nb_eval_steps += 1 eval_loss = eval_loss / nb_eval_steps pred_tags = [idx2tag[p_ii] for p in predictions for p_i in p for p_ii in p_i] joint_tokenized, joint_labels, preds = aggr_toks_labels_tags(sentence, labels, tok_sent, tok_labels, pred_tags) tmp = {&#39;word&#39;: sentence, &#39;orig_label&#39;: labels, &#39;predicted_tag&#39;: preds, &#39;corresToken&#39;: corres_tokens, &#39;sent_id&#39;: sent_id} tmp_df = pd.DataFrame(data=tmp) return tmp_df . full_dev_df = pd.DataFrame() dev_tokenized_texts, dev_tokenized_labels = tokenize(dev_sentences, dev_labels) for sent, label, tok_sent, tok_label, corresTokens, sent_id in zip(dev_sentences, dev_labels, dev_tokenized_texts, dev_tokenized_labels, dev_corresTokens, dev_sent_ids): eval_df = test_model(sent, label, tok_sent, tok_label, corresTokens, sent_id) full_dev_df = full_dev_df.append(eval_df, ignore_index=True, sort=False) . How the dev DataFrame looks like after evaluation . Under the &#39;predicted_tag&#39; column we can see the aggragated tag. Some processing is still required for the predicted tag to be comparable to the &#39;orig_label&#39; tag - see the next cells. . full_dev_df.head() . word orig_label predicted_tag corresToken sent_id . 0 עשרות | CDT | ^CD | 1 | 1 | . 1 אנשים | NN | ^NN | 2 | 1 | . 2 מגיעים | BN | ^BN^BN | 3 | 1 | . 3 מתאילנד | PREPOSITION^NNP | ^PREPOSITION^NNP^PREPOSITION^NNP^PREPOSITION^NNP | 4 | 1 | . 4 לישראל | PREPOSITION^NNP | ^PREPOSITION^NNP | 5 | 1 | . Splitting the predicted tag . As mentioned in the theoratical post and in the paper, we split the predicted tag to that it includes only unique tags by the order they were predicted. We make a list of tags from both the original tag and the predicted (uniquely filtered) tag, and those lists will be compared. . from more_itertools import unique_everseen def unique_vals_to_list(df): for index in df.index: joint_pred = df.at[index, &#39;predicted_tag&#39;] joint_orig = df.at[index, &#39;orig_label&#39;] predicted_tag_list = joint_pred.split(&#39;^&#39;) predicted_tag_list_no_empty = list(filter(None, predicted_tag_list)) original_tag_list = joint_orig.split(&#39;^&#39;) original_tag_list_no_empty = list(filter(None, original_tag_list)) df.at[index, &#39;predicted_tag&#39;] = list(unique_everseen(predicted_tag_list_no_empty)) df.at[index, &#39;orig_label&#39;] = list(unique_everseen(original_tag_list_no_empty)) unique_vals_to_list(full_dev_df) . Comparing predicted vs. original label . Two metrics of evaluation are available - exact match and existence. Exact match - pretty straight forward, an all-or-nothing approach, if the original label is IN^DEF^NN and the model predicted exactly that - great, if it predicted IN^NN - it doesn&#39;t count as success. This is a tough test, sure. What if the model actually predicted the tags but not in the right order (suppose DEF^IN^NN) or maybe just got 2 tags out of 3? doesn&#39;t that count for something? Sure thing. So the second metric is the existence, where we calculate the standard precision and recall for the tags that appeared in both the predicted and original tags and report the F1 score based on those calculations. . def exact_match_accuracy(df): exact_matches = 0 for index in df.index: if df.at[index, &#39;orig_label&#39;] == df.at[index, &#39;predicted_tag&#39;]: exact_matches += 1 return exact_matches print(&quot;DEV - Exact Match Accuracy = {0:.2f}%&quot;.format(exact_match_accuracy(dev_combined)/len(dev_combined) * 100)) . def existence_accuracy(df): # correct tag = appeared in predicted and in original total_orig_num_of_labels = 0 total_predicted_num_of_labels = 0 total_num_of_correct_tags = 0 for index in df.index: orig_list = df.at[index, &#39;orig_label&#39;] predicted_list = df.at[index, &#39;predicted_tag&#39;] total_orig_num_of_labels += len(orig_list) total_predicted_num_of_labels += len(predicted_list) total_num_of_correct_tags += len(set(orig_list).intersection(set(predicted_list))) precision = total_num_of_correct_tags / total_predicted_num_of_labels * 100 recall = total_num_of_correct_tags / total_orig_num_of_labels * 100 f1 = 2*precision*recall/(precision+recall) print(&quot;Precision: {0:.2f}%&quot;.format(precision)) print(&quot;Recall: {0:.2f}%&quot;.format(recall)) print(&quot;F1: {0:.2f}%&quot;.format(f1)) print(&quot;DEV:&quot;) existence_accuracy(dev_combined) . That&#39;s it! . Some more things that can be done (and maybe will be done by me in the future...) . Fine-tune on ordered sentences, that is, see if the learning improves if we apply some order to the sentences (for example arrange the sentences from shortest to longest). The idea behind this is that shorter sentences probably have a simpler syntactic structure, and fine-tuning on them first will hhelp the model learn basic and fundamental properties of Hebrew structure. | Change the tokenizer - I think this is the biggest issue in processing Hebrew in general. It was demonstrated in the paper that BERT&#39;s tokenizer is not suitable for languages with complex morphology - we should probably use a different tokenizer for such languages (but that would also require pre-training BERT all together). |",
            "url": "https://stavkl.github.io/linguistics-for-nlp/fastpages/jupyter/2020/10/22/sigmorphon-walkthrough.html",
            "relUrl": "/fastpages/jupyter/2020/10/22/sigmorphon-walkthrough.html",
            "date": " • Oct 22, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Getting the ###life out of Living: How adequate are word-pieces for modelling complex morphology?",
            "content": "This post reviews the paper I presented at SIGMORPHON this year from its theoretical perspective. During the work on this paper we got a lot of good comments on the theoretical side of things, and a lot of questions about the actual implementation and the fine-tuning process, so there’ll be another blog post with a walkthrough on one of the settings described here . . tl;dr . The task: Part-of-speech Tagging The language: Hebrew The data: The Hebrew treebank The means: fine-tuning Multilingual BERT So how adequate are word-pieces? not at all . The challenges with POS-tagging in Hebrew . This section only deals with the challenges in POS-tagging Hebrew, and in another post I’ll review the challenges of processing languages with non-concatenative morphology in general. In NLP, a word is a space-delimited sequence of characters. Each word is composed of at least one morpheme. A morpheme is the smallest unit of meaning, and it comes in two shapes: bound and free. A bound morpheme has to be part of a word, like the English plural suffix -s, while a free morpheme can stand on its own, like the coordinator and. . The main difference between English and Hebrew in this sense is that Hebrew has a much higher morphemes-to-word ratio than English, so for example the sequence and when I saw (4 free morphemes in English) is expressed in Hebrew using a single word - וכשראיתי /ve-kshe-raiti/ (and - bound, when - bound, saw.1st per.sg - free). Each morpheme in the Hebrew example has a different POS tag, which one should we choose to represent the whole word? Easy, we don’t choose, we take them all (otherwise we loose valuable syntactic information that we do encode for English). Introducing the multi-tag, which is a POS-tag composed of POS-tags. In the example the multi-tag would be CONJ^REL^VB. We use the ^ to denote that the order of the components is crucial, that is CONJ^REL^VB != CONJ^VB^REL (if such tag existed). . Trivia break! There is a single concept/meaning that is conveyed by a bound morpheme in English and a free morpheme in Hebrew (it’s usually the other way around!), can you find it? Hint in the comments. . Another major challenge in Hebrew is that some morphemes are covert (due to orthographic rules), so the internal structure of a word doesn’t necessarily correspond to the surface form. For example the word ב-בית /ba-bayit/ (‘in the house’) has two morphemes on the surface - in + house, and the definite article is covert (if there wasn’t a definite article it would be בבית /be-bayit/), but we don’t write the vowels in Hebrew script. . Lastly, Hebrew has an intertwined nature (a.k.a non-concatenative morphology), which for our purposes means that words can’t necessarily be segmented linearly. . Working through the challenges to find… more challenges! . The widely-accepted conclusion is that in order to parse Hebrew correctly we must first segment each word to its composing morphemes as part of a necessary pre-processing, and then we can continue with the regular pipeline like we do for English. However, it’s usually the case that a single word would have more than one possible segmentation, non of which is a-priori more likely, and the correct one is only recoverable in context…. . Along came BERT . Not going to introduce BERT here, it took the world by storm and since then has been used for pretty much anything - turning regular rocks into gold, curing the blind, bringing people back from the dead… and wasn’t trained on any of these tasks, what a guy! (Hi, this is me being totally sarcastic because it’s my blog. As you’ll see here and in the future, I’m not the biggest fan of huge-but-dumb models that happen to work well for English) BERT has four important qualities that make it interesting for multi-tagging Hebrew: . It’s a contextualized model (remember that the correct segmentation relies on context). | It segments words into sub-word units called word-pieces as part of its pre-processing (but uses a linear segmentation, more on that later). | Its multilingual version includes a Hebrew model which is trained on a larger corpus than what was used in previous pre-neural models for Hebrew. | It’s really important for reviewers. | And here’s where the study actually begins.. . After a long introduction we can now say that this study focuses mostly on the 2nd point above which is the segmentation process. We have established that we need segmentation of words into sub-word units, and by that we implicitly meant that we were looking for the correct segmentation (i.e. into the actual morphemes). But what if we can’t get the correct segmentation? it is kinda hard… Can we just use BERT’s segmentation into word-pieces (hence WPs) and get good enough results? . Hypothesis and Approach . The fact is that WPs do not reflect the actual morphemes, so we hypothesize that segmentation into WPs will deteriorate performance for multi-tagging Hebrew. By deteriorating performance we mean that either the accuracy levels will go down, or access to internal structure will be lost. We show how incorporating linguistic knowledge helps maintain access to internal structure as well as improving overall accuracy. . Some Experimental Settings . The running example throughout will be of the word בבית /’in the house’/, for which the relevant multi-tag is IN^DEF^NN and the BPE segmentation is ב, ##בית . Broadly speaking, because we only fine tune BERT and not changing the segmentation algorithm (something that would require pre-training from scratch) all we have left to play with are the tags that the WPs receive. There are two strategies, one where all the WPs of a given word get the same tag, and another where each WP can get a different tag. The common practice is to give all the WPs the same tag, so let’s start with that. . Predicting the entire multi-tag at word level . In this setting we predict for each WP the complete multi-tag of the entire word, that is, for the WP ‘ב’ predict the multi tag IN^DEF^NN and also for the second WP ‘##בית’. We choose the prediction of the first WP to be the multi-tag for the whole word. On the upside, this is the easiest method to apply, but nothing comes without a price - we cannot generalize to new unseen tags, so if the model haven’t seen the tag IN^DEF^NN during fine-tuning it won’t be able to predict it in the evaluation. Also, the inner structure of the word remain inaccessible - eventually the model’s prediction is that the word בבית has a multi-tag of IN^DEF^NN, but we can’t infer which part of the word contributed the NN tag, for example, we can only tell that it’s there. For POS tagging that might not make much of a difference, but for other tasks down the pipeline, like NER, it’s really important to recognize where a proper noun begins and ends, for example. The F1 score for this setting is 94.09. We can do better! We just need to be a little more sophisticated with the linguistic knowledge that we have and still didn’t use. . Predicting Prefix (multi)-tag and the host (multi)-tag Hebrew words, as described before, are composed of morphemes, some morphemes come before the stem (called prefixes, like dis- in English), and some come after (like -ness). While in English affixes don’t usually carry a separate POS-tag, in Hebrew they usually do. In this section we use the fact that we can separate, fairly easily, the prefix from the rest of the word (which we’ll call the host). For the record, separating the suffix is also possible but not as easily, so we didn’t do that. . We fine-tune the model twice, once for the prefix classifier and once for the host classifier. Here the prefix classifier only predicts prefixes - it predicts IN^DEF for both WPs, and the host classifier predicts NN for both WPs. We then combine the results to a single tag IN^DEF^NN. . Because we split and reassemble the tags we can generalize better to unseen tags, and we also get more access to the inner structure as the distinction between prefix and host is essentially a distinction between function and content (respectively), and it makes intuitive sense to fine-tune two models separately. It also yields a slightly better F1 score of 94.22. . Strategy Summary . So far we see that incorporating linguistic knowledge can help in both accuracy and access to internal structure. Next we’ll see what we can get when we step out of the common practices. . Predicting a single tag per word piece (decomposed) Moving on to the strategy where each WP can get a different tag. Let’s say you’re not entirely convinced that WPs don’t reflect morphology (even though I told you it’s a fact, they weren’t designed to do that), what will happen if we just predict a single, simple POS tag for each WP? The advantages are obvious, we’ll immediately know which WP contributed which tag, and the number of simple tags is really not that big (about 50) so we could generate any new multi-tag we’d like, right? Not exactly. The obvious advantages are still true, but it is also true that the overall F1 score drops to as low as 76.65, and that’s because we lose so much information (and therefore accuracy) to those damn covert morphemes! See the example in the figure and remember that the multi-tag of the word בבית is IN^DEF^NN, but the tag DEF is not (and can’t be) part of the output as it’s not attached to any WP. . (Multi-)tag per word piece (decomposed informed) So ideally we would want the first WP ב to reflect that its tag is IN but as a WP it also encapsulates the DEF tag, and we want the second WP בית to have just the tag NN. Essentially that means we want each WP to receive a (possibly multi-)tag that is composed only of the morphemes contained in the WP, covert or not. That way we get all the advantages from before, without falling to the covert morphemes pitfall. . So I wrote a function that does just that, based on the the data we have in the Hebrew Treebank and some linguistic traits and heuristics we know about the partial order of POS-tags in Hebrew and the covert nature of some POS-tags (namely the definite article). This setting gets an F1 score of 88.71. . Overall Settings Summary Overall, we saw that BERT’s segmentation of words into sub-word units might work well for English since the linguistic properties of English (namely its linear nature and low morphemes-to-word ratio) work well with the BPE algorithm that BERT uses. Unfortunately this doesn’t hold for Hebrew too, where we want the correct morphological segmentation. Using linguistic knowledge can help us overcome some of the challenges of processing Hebrew, but still there’s a trade-off between prediction accuracy and the access to the internal structure that is not yet solved. Also note that all the linguistic knowledge we incorporated was really only introduced to the model at the very late stage of fine-tuning, which is probably too late (all the representations were already pre-trained on unsuitable segmentation). I’m no longer sure this calls for a designated BERT-for-Hebrew model, like in other languages, but all of these need to be taken into consideration anyway. .",
            "url": "https://stavkl.github.io/linguistics-for-nlp/fastpages/jupyter/2020/09/21/getting-the-life.html",
            "relUrl": "/fastpages/jupyter/2020/09/21/getting-the-life.html",
            "date": " • Sep 21, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About",
          "content": "Hi :) . My name is Stav Klein. I’m studying towards an MA in Linguistics (Tel-Aviv University), and I’m also very interested in deep learning for NLP and especially in processing morphologically-complex and low-resource languages, first-language acquisition and how to use linguistic knowledge to improve models, methodologies and applications (“just use more data” is never the right answer). . Always happy to chat, you can find me on Twitter, Linkedin or Github . Moving on up",
          "url": "https://stavkl.github.io/linguistics-for-nlp/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
      ,"page3": {
          "title": "Publications",
          "content": "2020 . Notes on Modern Hebrew phonology and orthography (book chapter)Stav Klein. Usage-Based Studies in Modern Hebrew - Background, Morpho-lexicon and Syntax. . Getting the ##life out of living: How adequate are word-pieces for modelling complex morphology?Stav Klein and Reut Tsarfaty. SIGMORPHON, ACL workshop. . From SPMRL to NMRL: What Did We Learn (and Unlearn) in a Decade of Parsing Morphologically-Rich Languages (MRLs)?Reut Tsarfaty, Dan Bareket, Stav Klein, Amit Seker. ACL. . 2019 . What’s Wrong with Hebrew NLP? And How to Make it RightReut Tsarfaty, Shoval Sadde, Stav Klein, Amit Seker. EMNLP Demo paper. .",
          "url": "https://stavkl.github.io/linguistics-for-nlp/publications/",
          "relUrl": "/publications/",
          "date": ""
      }
      
  

  
  

  

  
  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://stavkl.github.io/linguistics-for-nlp/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}