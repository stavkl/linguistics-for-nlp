<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="twitter:card" content="summary_large_image" /><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>HeBERT vs. mBERT | Linguistics for NLP</title>
<meta name="generator" content="Jekyll v4.0.0" />
<meta property="og:title" content="HeBERT vs. mBERT" />
<meta name="author" content="Stav Klein" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Testing the new model for Hebrew on cPOS-tagging" />
<meta property="og:description" content="Testing the new model for Hebrew on cPOS-tagging" />
<link rel="canonical" href="https://stavkl.github.io/linguistics-for-nlp/fastpages/jupyter/2020/12/27/oscar-bert-model.html" />
<meta property="og:url" content="https://stavkl.github.io/linguistics-for-nlp/fastpages/jupyter/2020/12/27/oscar-bert-model.html" />
<meta property="og:site_name" content="Linguistics for NLP" />
<meta property="og:image" content="https://stavkl.github.io/linguistics-for-nlp/images/oscar-bert.png" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-12-27T00:00:00-06:00" />
<script type="application/ld+json">
{"author":{"@type":"Person","name":"Stav Klein"},"description":"Testing the new model for Hebrew on cPOS-tagging","headline":"HeBERT vs. mBERT","dateModified":"2020-12-27T00:00:00-06:00","datePublished":"2020-12-27T00:00:00-06:00","@type":"BlogPosting","image":"https://stavkl.github.io/linguistics-for-nlp/images/oscar-bert.png","mainEntityOfPage":{"@type":"WebPage","@id":"https://stavkl.github.io/linguistics-for-nlp/fastpages/jupyter/2020/12/27/oscar-bert-model.html"},"url":"https://stavkl.github.io/linguistics-for-nlp/fastpages/jupyter/2020/12/27/oscar-bert-model.html","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/linguistics-for-nlp/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://stavkl.github.io/linguistics-for-nlp/feed.xml" title="Linguistics for NLP" /><link rel="shortcut icon" type="image/x-icon" href="/linguistics-for-nlp/images/favicon.ico"><link href="https://unpkg.com/@primer/css/dist/primer.css" rel="stylesheet" />
<link rel="stylesheet" href="//use.fontawesome.com/releases/v5.0.7/css/all.css">

<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/linguistics-for-nlp/">Linguistics for NLP</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/linguistics-for-nlp/about/">About</a><a class="page-link" href="/linguistics-for-nlp/publications/">Publications</a><a class="page-link" href="/linguistics-for-nlp/search/">Search</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">HeBERT vs. mBERT</h1><p class="page-description">Testing the new model for Hebrew on cPOS-tagging</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2020-12-27T00:00:00-06:00" itemprop="datePublished">
        Dec 27, 2020
      </time>• 
          <span itemprop="author" itemscope itemtype="http://schema.org/Person">
            <span class="p-author h-card" itemprop="name">Stav Klein</span></span>
       • <span class="read-time" title="Estimated read time">
    
    
      3 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/linguistics-for-nlp/categories/#fastpages">fastpages</a>
        &nbsp;
      
        <a class="category-tags-link" href="/linguistics-for-nlp/categories/#jupyter">jupyter</a>
        
      
      </p>
    

    
      
        <div class="pb-5 d-flex flex-wrap flex-justify-end">
          <div class="px-2">

    <a href="https://github.com/stavkl/linguistics-for-nlp/tree/master/_notebooks/2020-12-27-oscar-bert-model.ipynb" role="button" target="_blank">
<img class="notebook-badge-image" src="/linguistics-for-nlp/assets/badges/github.svg" alt="View On GitHub">
    </a>
</div>

          <div class="px-2">
    <a href="https://mybinder.org/v2/gh/stavkl/linguistics-for-nlp/master?filepath=_notebooks%2F2020-12-27-oscar-bert-model.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/linguistics-for-nlp/assets/badges/binder.svg" alt="Open In Binder"/>
    </a>
</div>

          <div class="px-2">
    <a href="https://colab.research.google.com/github/stavkl/linguistics-for-nlp/blob/master/_notebooks/2020-12-27-oscar-bert-model.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/linguistics-for-nlp/assets/badges/colab.svg" alt="Open In Colab"/>
    </a>
</div>
        </div>
      </header>

  <div class="post-content e-content" itemprop="articleBody">
    <!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2020-12-27-oscar-bert-model.ipynb
-->

<div class="container" id="notebook-container">
        
    
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Data-Overview">Data Overview<a class="anchor-link" href="#Data-Overview"> </a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Avichay Chriqui and Dr. Inbal Yahav Shenberger recently released a new model for Hebrew based on BERT's architecture and trained on the OSCAR corpus. The model is built with the huggingface library and uses the library's <code>AutoTokenizer</code> (which is WordPieces since the model is BERT-based) and the model itself is <code>AutoModelForMaskedLM</code>. 
Key differences between HeBERT and mBERT are summarized in the table below:</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea output_execute_result">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>HeBERT</th>
      <th>mBERT</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th># of word-pieces</th>
      <td>~ 30K</td>
      <td>~ 2K</td>
    </tr>
    <tr>
      <th>Training Data</th>
      <td>Hebrew Wiki, Hebrew OSCAR, User-generated content</td>
      <td>Hebrew Wiki</td>
    </tr>
  </tbody>
</table>
</div>
</div>

</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea output_execute_result">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Size</th>
      <th># of sentences</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>Hebrew Wiki</th>
      <td>650MB</td>
      <td>3.8M</td>
    </tr>
    <tr>
      <th>Hebrew OSCAR</th>
      <td>9.8GB</td>
      <td>20.8M</td>
    </tr>
    <tr>
      <th>UGC</th>
      <td>150MB</td>
      <td>350K</td>
    </tr>
  </tbody>
</table>
</div>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>It's clear from both tables that the HeBERT is order of magnitude larger than the Hebrew part of the multilingual BERT. I reproduced two experimental settings from this <a href="https://www.aclweb.org/anthology/2020.sigmorphon-1.24/">paper from 2020 SIGMORPHON</a> (and see also the corresponding <a href="https://stavkl.github.io/linguistics-for-nlp/fastpages/jupyter/2020/09/21/getting-the-life.html">blog post</a> for more details) in order to examine the contribution of huge amounts of data to the complex-POS tagging task. Both experiments ran in exactly the same settings, with the only differences being the choice of model and tokenizer.<br />
Below are examples for the tokenization differences between models:<br />
<strong>mBERT tokenization</strong><br />
<img src="https://github.com/stavkl/linguistics-for-nlp/raw/master/images/tokenized-examples/mbert-tokenized.PNG" alt="" /><br />
<strong>HeBERT tokenization</strong><br />
<img src="https://github.com/stavkl/linguistics-for-nlp/raw/master/images/tokenized-examples/hebert-tokenized.PNG" alt="" /><br /></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Experiment-1:-Word-level-Multitag">Experiment 1: Word-level Multitag<a class="anchor-link" href="#Experiment-1:-Word-level-Multitag"> </a></h3>
</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea output_execute_result">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>HeBERT</th>
      <th>mBERT</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>Exact Match</th>
      <td>94.63</td>
      <td>92.45</td>
    </tr>
    <tr>
      <th>F1</th>
      <td>96.13</td>
      <td>94.09</td>
    </tr>
  </tbody>
</table>
</div>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>This setting shows an improvement in both exact-match accuracy and existence-f1 measures. It indicates that for settings where the access to the inner structure is not crucial it might be better to use a larger model. Nevertheless, the results received for HeBERT here are still on par with models like <a href="https://https://www.aclweb.org/anthology/Q19-1003.pdf">YAP</a> (and needless to say, YAP is actually tested on a harder problem, so it's not really a like-for-like comparison).</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Experiment-2:-(Multi)-tag-per-Wordpiece">Experiment 2: (Multi)-tag per Wordpiece<a class="anchor-link" href="#Experiment-2:-(Multi)-tag-per-Wordpiece"> </a></h3><p>Recall that in this setting each wordpiece can receive a possibly different tag or multi-tag. The results show that for a significantly larger model the procedure of assigning each wordpiece a different tag eventually deteriorates performance, probably because this procedure is based on heuristics about Hebrew morphology and also because it was designed with many wordpieces in mind while in HeBERT many words don't break into wordpieces.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea output_execute_result">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>HeBERT</th>
      <th>mBERT</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>Exact Match</th>
      <td>84.86</td>
      <td>86.66</td>
    </tr>
    <tr>
      <th>F1</th>
      <td>84.62</td>
      <td>88.71</td>
    </tr>
  </tbody>
</table>
</div>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="My-linguistic-take-on-things:-Using-huge-models-for-Hebrew-is-like-solving-a-100-piece-puzzle-using-30,000-pieces">My linguistic take on things: Using huge models for Hebrew is like solving a 100-piece puzzle using 30,000 pieces<a class="anchor-link" href="#My-linguistic-take-on-things:-Using-huge-models-for-Hebrew-is-like-solving-a-100-piece-puzzle-using-30,000-pieces"> </a></h3><p>The results signify that for tasks that don't require specific knowledge about internal structure it might be better to use a significantly larger model like HeBERT. However, this improvement is not big and it seems possible to achive that much improvement by taking other measures on the original mBERT (and avoid the huge training cost altogether). Here are some ideas:</p>
<ol>
<li>Change the tokenizer - I think that's the single most meaningful thing that can be done for processing Hebrew (and other semitic languages). It was already shown that wordpieces are useless for modelling complex morphology.</li>
<li>Also related to that - I hypothesize that Hebrew models can use a much smaller vocabulary. Hebrew is really more like a 100-piece puzzle in the sense that many morphemes have a well-defined purpose and they connect to the other morphemes in very specific ways. Having 30K pieces actually makes the puzzle much harder to solve. Improving the model's "knowledge" of the affixation processes can reduce training costs and might lead to better morphological disambiguation and so to other improvements down the pipeline.</li>
<li>Imposing some structure on the vocabulary, instead of just going over a huge list looking for the largest wordpiece we can fit. This would require some rule-based intervention with the process but might be worth checking.</li>
<li>Changing the task - LM predicts the next word in a sentence, which works great for English since English sentences exhibit a strict word order, and so by learning what the next word is we also encode syntactic roles. This is not the case for Hebrew as word-order can vary a lot. However, the internal structure of a Hebrew word does NOT vary a lot, so we can try transferring "next-word prediction" to the character level to capture more morphological knowledge (though it would not encode the non-concatenative template, that remains a problem).</li>
</ol>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>That's all for now,<br />
until next time<br />
Stav</p>

</div>
</div>
</div>
</div>



  </div><!-- from https://github.com/utterance/utterances -->
<script src="https://utteranc.es/client.js"
        repo="stavkl/linguistics-for-nlp"
        issue-term="title"
        label="blogpost-comment"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script><a class="u-url" href="/linguistics-for-nlp/fastpages/jupyter/2020/12/27/oscar-bert-model.html" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/linguistics-for-nlp/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/linguistics-for-nlp/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/linguistics-for-nlp/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>My linguistic perspective on NLP research.</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/stavkl" title="stavkl"><svg class="svg-icon grey"><use xlink:href="/linguistics-for-nlp/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://twitter.com/stavkl" title="stavkl"><svg class="svg-icon grey"><use xlink:href="/linguistics-for-nlp/assets/minima-social-icons.svg#twitter"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
